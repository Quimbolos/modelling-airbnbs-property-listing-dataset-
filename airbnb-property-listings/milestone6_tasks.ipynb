{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the DataSet\n",
    "\n",
    "from tabular_data import load_airbnb\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_airbnb()\n",
    "X.drop(532, axis=0, inplace=True)\n",
    "y.drop(532, axis=0, inplace=True)\n",
    "X['guests'] = X['guests'].str.replace('\\'','').astype(np.float64)\n",
    "X['bedrooms'] = X['bedrooms'].str.replace('\\'','').astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2.0000, 1.0000, 1.0000, 5.0000, 5.0000, 4.8000, 5.0000, 5.0000, 4.8000,\n",
      "        8.0000, 1.0000], dtype=torch.float64), tensor(126))\n",
      "889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(623, 133, 133)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Task 1\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DataSet Class\n",
    "class AirbnbNightlyPriceImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.X, self.y = X , y\n",
    "    # Not dependent on index\n",
    "    def __getitem__(self, index):\n",
    "        features = torch.tensor(self.X.iloc[index])\n",
    "        label = torch.tensor(self.y.iloc[index])\n",
    "        return (features, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "dataset = AirbnbNightlyPriceImageDataset()\n",
    "print(dataset[10])\n",
    "print(len(dataset))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Split the data \n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader=DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/r5th0fxd26341bbn6qkwvlww0000gn/T/ipykernel_6344/3374604862.py:25: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(prediction, labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n",
      "tensor(6225.5342, grad_fn=<MseLossBackward0>)\n",
      "tensor(40599.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(80.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(731.9358, grad_fn=<MseLossBackward0>)\n",
      "tensor(41494.6133, grad_fn=<MseLossBackward0>)\n",
      "tensor(32702.2871, grad_fn=<MseLossBackward0>)\n",
      "tensor(181890.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8752.2373, grad_fn=<MseLossBackward0>)\n",
      "tensor(146546.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(130453.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(5220.3545, grad_fn=<MseLossBackward0>)\n",
      "tensor(23076.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(18732.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(19343.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(51325.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(12476.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(3886.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor(89861.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(9649.5410, grad_fn=<MseLossBackward0>)\n",
      "tensor(5787.6255, grad_fn=<MseLossBackward0>)\n",
      "tensor(132155.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4180.6704, grad_fn=<MseLossBackward0>)\n",
      "tensor(29868.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(29760.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(67687.5781, grad_fn=<MseLossBackward0>)\n",
      "tensor(5003.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1240.9252, grad_fn=<MseLossBackward0>)\n",
      "tensor(23905.2383, grad_fn=<MseLossBackward0>)\n",
      "tensor(9758.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(7415.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(6352.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(31604.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(33413.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(3096.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(6985.3589, grad_fn=<MseLossBackward0>)\n",
      "tensor(184161.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(9561.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor(85842.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1686.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(15882.9551, grad_fn=<MseLossBackward0>)\n",
      "tensor(16693.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(26217.2207, grad_fn=<MseLossBackward0>)\n",
      "tensor(12615.6387, grad_fn=<MseLossBackward0>)\n",
      "tensor(12733.1924, grad_fn=<MseLossBackward0>)\n",
      "tensor(3608.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4220.6006, grad_fn=<MseLossBackward0>)\n",
      "tensor(10754.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor(14974.3916, grad_fn=<MseLossBackward0>)\n",
      "tensor(13551.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1259100.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(20288.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(14023.4727, grad_fn=<MseLossBackward0>)\n",
      "tensor(52855.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(104.1872, grad_fn=<MseLossBackward0>)\n",
      "tensor(3220.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor(13994.2080, grad_fn=<MseLossBackward0>)\n",
      "tensor(4837.9106, grad_fn=<MseLossBackward0>)\n",
      "tensor(54864.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(6612.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor(10251.4053, grad_fn=<MseLossBackward0>)\n",
      "tensor(20695.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(5935.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(13136.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(606.7131, grad_fn=<MseLossBackward0>)\n",
      "tensor(15847.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor(4324.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(19641.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1678.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(369.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(45003.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(40646.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(13935.5566, grad_fn=<MseLossBackward0>)\n",
      "tensor(246933.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(21457.7168, grad_fn=<MseLossBackward0>)\n",
      "tensor(17696.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(16603.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(14810.6328, grad_fn=<MseLossBackward0>)\n",
      "tensor(27869.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(4387.9082, grad_fn=<MseLossBackward0>)\n",
      "tensor(38702.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(601.6028, grad_fn=<MseLossBackward0>)\n",
      "tensor(10194.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor(21438.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(2033.8695, grad_fn=<MseLossBackward0>)\n",
      "tensor(1699.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7678.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(182095.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(12208.6211, grad_fn=<MseLossBackward0>)\n",
      "tensor(10735.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(4068.7043, grad_fn=<MseLossBackward0>)\n",
      "tensor(5582.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(20027.6348, grad_fn=<MseLossBackward0>)\n",
      "tensor(1941.1606, grad_fn=<MseLossBackward0>)\n",
      "tensor(19571.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(6369.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(5987.6016, grad_fn=<MseLossBackward0>)\n",
      "tensor(15425.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(9203.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1264.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2361.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(21823.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(3332.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(93693.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(5779.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(2910.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(10911.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(14238.6445, grad_fn=<MseLossBackward0>)\n",
      "tensor(11002.9463, grad_fn=<MseLossBackward0>)\n",
      "tensor(5810.5586, grad_fn=<MseLossBackward0>)\n",
      "tensor(56101.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(48866.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(3302.4226, grad_fn=<MseLossBackward0>)\n",
      "tensor(19688.7832, grad_fn=<MseLossBackward0>)\n",
      "tensor(57514.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(7493.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(11875.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor(5888.6338, grad_fn=<MseLossBackward0>)\n",
      "tensor(13799.2939, grad_fn=<MseLossBackward0>)\n",
      "tensor(25275.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(5046.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(439753.2188, grad_fn=<MseLossBackward0>)\n",
      "tensor(15485.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor(7429.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(3400.3315, grad_fn=<MseLossBackward0>)\n",
      "tensor(333561.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2855.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(9756.7783, grad_fn=<MseLossBackward0>)\n",
      "tensor(30870.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(40510.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(21659.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(16545.8398, grad_fn=<MseLossBackward0>)\n",
      "tensor(7708.3584, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "\n",
    "#Linear Model\n",
    "class LinearRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialise the Parameters\n",
    "        self.linear_layer = torch.nn.Linear(11,1) # 11 features, 1 label\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Use the layers to process the features\n",
    "        return self.linear_layer(features)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train function\n",
    "def train(model, dataloader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = F.mse_loss(prediction, labels)\n",
    "            loss.backward()\n",
    "            print(loss)\n",
    "            \n",
    "    return\n",
    "\n",
    "train(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45433.6406, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4612e+11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6975e+11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1447e+12, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6102e+12, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3460e+13, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2160e+14, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0976e+15, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6695e+15, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8985e+16, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2945e+17, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2299e+18, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7375e+18, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3610e+19, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9154e+20, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0331e+21, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2848e+21, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8198e+22, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4980e+22, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1974e+23, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0577e+24, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3810e+24, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6671e+25, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3703e+25, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8883e+26, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1275e+26, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9405e+27, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5586e+28, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3287e+28, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9355e+29, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6891e+30, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1827e+31, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3056e+31, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0458e+32, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2901e+32, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6504e+33, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5659e+34, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9728e+34, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2003e+35, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(inf, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquimbolosfernandez/miniforge3/envs/Tensorflow/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/joaquimbolosfernandez/miniforge3/envs/Tensorflow/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Task 3 \n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() # This Loss function is better\n",
    "\n",
    "# Train function with optimiser\n",
    "def train(model, dataloader, epochs=10):\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction, labels)\n",
    "            loss.backward() # What does this do? Populates the gradients?\n",
    "            print(loss)\n",
    "            optimiser.step() # Optimiser step\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "    return\n",
    "\n",
    "train(model,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 32874.046875\n",
      "Loss 33385.96484375\n",
      "Loss 18677.79296875\n",
      "Loss 6260.33251953125\n",
      "Loss 29972.0390625\n",
      "Loss 152446.515625\n",
      "Loss 51624.62890625\n",
      "Loss 13705.05859375\n",
      "Loss 17883.490234375\n",
      "Loss 38810.6171875\n",
      "Loss 33240.375\n",
      "Loss 21057.158203125\n",
      "Loss 21708.265625\n",
      "Loss 14642.0244140625\n",
      "Loss 55883.1796875\n",
      "Loss 50960.8046875\n",
      "Loss 23199.65625\n",
      "Loss 21984.55859375\n",
      "Loss 42962.0625\n",
      "Loss 65052.875\n",
      "Loss 44503.12890625\n",
      "Loss 27910.4453125\n",
      "Loss 51964.23828125\n",
      "Loss 94222.703125\n",
      "Loss 61028.0078125\n",
      "Loss 23614.3125\n",
      "Loss 16019.2275390625\n",
      "Loss 35824.36328125\n",
      "Loss 14794.5458984375\n",
      "Loss 49328.046875\n",
      "Loss 18435.94140625\n",
      "Loss 31823.076171875\n",
      "Loss 27371.291015625\n",
      "Loss 74436.7890625\n",
      "Loss 16412.806640625\n",
      "Loss 34907.51953125\n",
      "Loss 21182.865234375\n",
      "Loss 48917.1640625\n",
      "Loss 21511.837890625\n",
      "Loss 35571.33984375\n",
      "Loss 15856.921875\n",
      "Loss 52315.2734375\n",
      "Loss 36915.86328125\n",
      "Loss 16096.279296875\n",
      "Loss 45267.6015625\n",
      "Loss 22541.26171875\n",
      "Loss 38462.34765625\n",
      "Loss 16142.451171875\n",
      "Loss 14949.314453125\n",
      "Loss 28321.900390625\n",
      "Loss 87435.9140625\n",
      "Loss 23038.40234375\n",
      "Loss 36630.90234375\n",
      "Loss 26662.072265625\n",
      "Loss 37120.328125\n",
      "Loss 20256.81640625\n",
      "Loss 15097.458984375\n",
      "Loss 46610.55859375\n",
      "Loss 83811.625\n",
      "Loss 14429.0771484375\n",
      "Loss 26372.841796875\n",
      "Loss 46507.265625\n",
      "Loss 13925.09375\n",
      "Loss 32154.265625\n",
      "Loss 24741.72265625\n",
      "Loss 23448.916015625\n",
      "Loss 41617.890625\n",
      "Loss 21705.51171875\n",
      "Loss 74165.984375\n",
      "Loss 29034.955078125\n",
      "Loss 16450.044921875\n",
      "Loss 33258.33203125\n",
      "Loss 21301.46875\n",
      "Loss 82354.6640625\n",
      "Loss 35056.640625\n",
      "Loss 20901.5\n",
      "Loss 63211.85546875\n",
      "Loss 45213.1484375\n",
      "Loss 53475.88671875\n",
      "Loss 34205.73046875\n",
      "Loss 22497.9375\n",
      "Loss 31431.767578125\n",
      "Loss 57041.75390625\n",
      "Loss 18001.380859375\n",
      "Loss 60098.6640625\n",
      "Loss 30824.306640625\n",
      "Loss 67237.640625\n",
      "Loss 44232.0703125\n",
      "Loss 25181.806640625\n",
      "Loss 80313.09375\n",
      "Loss 16889.783203125\n",
      "Loss 43687.1640625\n",
      "Loss 26631.943359375\n",
      "Loss 21614.689453125\n",
      "Loss 34057.42578125\n",
      "Loss 19612.255859375\n",
      "Loss 23165.3671875\n",
      "Loss 33161.81640625\n",
      "Loss 21293.185546875\n",
      "Loss 33051.3515625\n",
      "Loss 24511.90625\n",
      "Loss 21383.056640625\n",
      "Loss 16954.455078125\n",
      "Loss 37090.46484375\n",
      "Loss 11598.83984375\n",
      "Loss 22681.310546875\n",
      "Loss 22014.04296875\n",
      "Loss 28670.482421875\n",
      "Loss 68209.3515625\n",
      "Loss 14710.373046875\n",
      "Loss 26544.28125\n",
      "Loss 54830.77734375\n",
      "Loss 43426.81640625\n",
      "Loss 30334.75390625\n",
      "Loss 59928.20703125\n",
      "Loss 55032.74609375\n",
      "Loss 14421.2744140625\n",
      "Loss 42015.55078125\n",
      "Loss 37675.734375\n",
      "Loss 35698.203125\n",
      "Loss 20224.546875\n",
      "Loss 49576.171875\n",
      "Loss 83219.5\n",
      "Loss 26907.265625\n",
      "Loss 28460.552734375\n",
      "Loss 29286.26953125\n",
      "Loss 30674.4921875\n",
      "Loss 18502.5859375\n",
      "Loss 19956.982421875\n",
      "Loss 20095.03515625\n",
      "Loss 12529.7392578125\n",
      "Loss 32851.671875\n",
      "Loss 68815.4453125\n",
      "Loss 16555.21484375\n",
      "Loss 63831.41796875\n",
      "Loss 38109.75\n",
      "Loss 32420.845703125\n",
      "Loss 27948.201171875\n",
      "Loss 38293.94921875\n",
      "Loss 33569.54296875\n",
      "Loss 19082.142578125\n",
      "Loss 23634.71875\n",
      "Loss 51736.078125\n",
      "Loss 11169.6669921875\n",
      "Loss 49218.95703125\n",
      "Loss 32725.984375\n",
      "Loss 62927.75390625\n",
      "Loss 26894.859375\n",
      "Loss 27986.34765625\n",
      "Loss 39198.37109375\n",
      "Loss 42281.734375\n",
      "Loss 20309.048828125\n",
      "Loss 27952.81640625\n",
      "Loss 21591.744140625\n",
      "Loss 48628.1875\n",
      "Loss 25409.392578125\n",
      "Loss 50726.16015625\n",
      "Loss 35330.54296875\n",
      "Loss 34335.63671875\n",
      "Loss 62174.609375\n",
      "Loss 22530.564453125\n",
      "Loss 15980.4658203125\n",
      "Loss 14020.9375\n",
      "Loss 10581.5908203125\n",
      "Loss 28232.119140625\n",
      "Loss 34099.54296875\n",
      "Loss 20869.693359375\n",
      "Loss 22023.689453125\n",
      "Loss 34031.15625\n",
      "Loss 25221.4453125\n",
      "Loss 23300.296875\n",
      "Loss 42368.42578125\n",
      "Loss 21713.197265625\n",
      "Loss 99777.140625\n",
      "Loss 29409.53515625\n",
      "Loss 38153.0625\n",
      "Loss 19732.734375\n",
      "Loss 50632.8203125\n",
      "Loss 23278.1953125\n",
      "Loss 40713.7578125\n",
      "Loss 42683.35546875\n",
      "Loss 17386.767578125\n",
      "Loss 21508.5859375\n",
      "Loss 15093.9638671875\n",
      "Loss 22253.93359375\n",
      "Loss 67433.8984375\n",
      "Loss 19294.396484375\n",
      "Loss 31351.927734375\n",
      "Loss 38212.92578125\n",
      "Loss 84824.5546875\n",
      "Loss 59684.421875\n",
      "Loss 32651.755859375\n",
      "Loss 13061.990234375\n",
      "Loss 23561.4921875\n",
      "Loss 37934.65625\n",
      "Loss 43649.1171875\n",
      "Loss 48346.3671875\n",
      "Loss 46880.58984375\n",
      "Loss 21618.779296875\n",
      "Loss 32633.884765625\n",
      "Loss 19461.517578125\n",
      "Loss 43342.23828125\n",
      "Loss 17238.298828125\n",
      "Loss 60250.98828125\n",
      "Loss 56358.109375\n",
      "Loss 27173.1328125\n",
      "Loss 32957.35546875\n",
      "Loss 69711.3359375\n",
      "Loss 10080.2021484375\n",
      "Loss 28146.24609375\n",
      "Loss 36023.8671875\n",
      "Loss 38023.890625\n",
      "Loss 15102.154296875\n",
      "Loss 20760.302734375\n",
      "Loss 21226.81640625\n",
      "Loss 39527.94140625\n",
      "Loss 16662.06640625\n",
      "Loss 14500.9970703125\n",
      "Loss 69146.7109375\n",
      "Loss 27929.94921875\n",
      "Loss 41396.8359375\n",
      "Loss 22613.046875\n",
      "Loss 62850.20703125\n",
      "Loss 59841.14453125\n",
      "Loss 42176.59765625\n",
      "Loss 14881.2548828125\n",
      "Loss 16722.734375\n",
      "Loss 15975.4697265625\n",
      "Loss 24117.388671875\n",
      "Loss 10269.87109375\n",
      "Loss 52990.734375\n",
      "Loss 25306.07421875\n",
      "Loss 50541.87890625\n",
      "Loss 15983.986328125\n",
      "Loss 30032.466796875\n",
      "Loss 20945.197265625\n",
      "Loss 25638.013671875\n",
      "Loss 27748.67578125\n",
      "Loss 33566.4609375\n",
      "Loss 21114.642578125\n",
      "Loss 25745.568359375\n",
      "Loss 17829.83984375\n",
      "Loss 31731.572265625\n",
      "Loss 44502.0234375\n",
      "Loss 78352.5390625\n",
      "Loss 19906.46484375\n",
      "Loss 35304.8828125\n",
      "Loss 17923.40234375\n",
      "Loss 15549.166015625\n",
      "Loss 37379.69921875\n",
      "Loss 10919.8173828125\n",
      "Loss 16681.99609375\n",
      "Loss 21897.60546875\n",
      "Loss 26110.33203125\n",
      "Loss 82575.2578125\n",
      "Loss 62299.1484375\n",
      "Loss 65107.6640625\n",
      "Loss 34091.5625\n",
      "Loss 41767.3203125\n",
      "Loss 21785.1328125\n",
      "Loss 23889.3828125\n",
      "Loss 18217.10546875\n",
      "Loss 13778.583984375\n",
      "Loss 19788.462890625\n",
      "Loss 36552.90625\n",
      "Loss 55669.296875\n",
      "Loss 23795.13671875\n",
      "Loss 64911.53125\n",
      "Loss 21005.685546875\n",
      "Loss 34811.390625\n",
      "Loss 23231.0234375\n",
      "Loss 74337.734375\n",
      "Loss 24255.93359375\n",
      "Loss 19201.38671875\n",
      "Loss 15282.65625\n",
      "Loss 45210.4765625\n",
      "Loss 20395.07421875\n",
      "Loss 32196.20703125\n",
      "Loss 22149.2890625\n",
      "Loss 25526.12890625\n",
      "Loss 30038.23828125\n",
      "Loss 22528.828125\n",
      "Loss 19142.955078125\n",
      "Loss 46770.94921875\n",
      "Loss 35300.75390625\n",
      "Loss 26782.115234375\n",
      "Loss 17043.369140625\n",
      "Loss 15363.3330078125\n",
      "Loss 16334.630859375\n",
      "Loss 48601.44921875\n",
      "Loss 31069.033203125\n",
      "Loss 97674.75\n",
      "Loss 27239.90234375\n",
      "Loss 25951.55859375\n",
      "Loss 24402.642578125\n",
      "Loss 51568.65234375\n",
      "Loss 23684.525390625\n",
      "Loss 20034.7421875\n",
      "Loss 44588.21875\n",
      "Loss 14599.7216796875\n",
      "Loss 19455.1328125\n",
      "Loss 102221.8515625\n",
      "Loss 25328.0234375\n",
      "Loss 24456.82421875\n",
      "Loss 24138.05078125\n",
      "Loss 55689.765625\n",
      "Loss 47430.86328125\n",
      "Loss 57192.7578125\n",
      "Loss 12150.7998046875\n",
      "Loss 31612.353515625\n",
      "Loss 34984.38671875\n",
      "Loss 36153.86328125\n",
      "Loss 20050.83203125\n",
      "Loss 15408.65625\n",
      "Loss 43156.21875\n",
      "Loss 25259.591796875\n",
      "Loss 13819.150390625\n",
      "Loss 96295.6015625\n",
      "Loss 33372.4375\n",
      "Loss 57488.0859375\n",
      "Loss 8426.6982421875\n",
      "Loss 14348.912109375\n",
      "Loss 14969.4853515625\n",
      "Loss 27114.49609375\n",
      "Loss 29743.810546875\n",
      "Loss 31205.60546875\n",
      "Loss 27026.53125\n",
      "Loss 14789.98046875\n",
      "Loss 16914.123046875\n",
      "Loss 45097.84375\n",
      "Loss 66808.40625\n",
      "Loss 84868.734375\n",
      "Loss 64090.51953125\n",
      "Loss 19256.984375\n",
      "Loss 48486.1328125\n",
      "Loss 32382.55078125\n",
      "Loss 29472.626953125\n",
      "Loss 49429.69921875\n",
      "Loss 23848.89453125\n",
      "Loss 20910.669921875\n",
      "Loss 18907.0390625\n",
      "Loss 52625.12890625\n",
      "Loss 23470.65234375\n",
      "Loss 27305.044921875\n",
      "Loss 15232.6181640625\n",
      "Loss 11865.556640625\n",
      "Loss 26695.525390625\n",
      "Loss 25169.755859375\n",
      "Loss 30111.396484375\n",
      "Loss 35453.89453125\n",
      "Loss 36842.81640625\n",
      "Loss 41686.984375\n",
      "Loss 19877.16796875\n",
      "Loss 21254.52734375\n",
      "Loss 19460.11328125\n",
      "Loss 81300.6640625\n",
      "Loss 46239.85546875\n",
      "Loss 68328.328125\n",
      "Loss 11398.2705078125\n",
      "Loss 72681.84375\n",
      "Loss 53974.08203125\n",
      "Loss 42988.453125\n",
      "Loss 15445.18359375\n",
      "Loss 67479.953125\n",
      "Loss 39027.40234375\n",
      "Loss 27323.626953125\n",
      "Loss 33950.41796875\n",
      "Loss 32359.984375\n",
      "Loss 16294.7724609375\n",
      "Loss 19441.712890625\n",
      "Loss 18239.42578125\n",
      "Loss 19616.541015625\n",
      "Loss 24436.2578125\n",
      "Loss 9908.9580078125\n",
      "Loss 15557.9111328125\n",
      "Loss 14760.48828125\n",
      "Loss 31715.16015625\n",
      "Loss 23920.97265625\n",
      "Loss 18534.279296875\n",
      "Loss 65145.73828125\n",
      "Loss 69869.328125\n",
      "Loss 26844.828125\n",
      "Loss 25513.08203125\n",
      "Loss 22420.962890625\n",
      "Loss 17106.5703125\n",
      "Loss 34384.19140625\n",
      "Loss 21003.19140625\n",
      "Loss 28006.4140625\n",
      "Loss 20772.30078125\n",
      "Loss 27218.79296875\n",
      "Loss 74677.5625\n",
      "Loss 30009.4609375\n",
      "Loss 17986.05859375\n",
      "Loss 64912.56640625\n",
      "Loss 34177.62890625\n",
      "Loss 42977.9453125\n",
      "Loss 13262.6943359375\n",
      "Loss 53677.9296875\n",
      "Loss 19088.794921875\n",
      "Loss 32808.11328125\n",
      "Loss 23466.22265625\n",
      "Loss 41526.2265625\n",
      "Loss 39498.15625\n",
      "Loss 19718.2578125\n",
      "Loss 22576.2734375\n",
      "Loss 45462.31640625\n",
      "Loss 48692.4140625\n",
      "Loss 19017.76171875\n",
      "Loss 18319.232421875\n",
      "Loss 68448.1953125\n",
      "Loss 15131.8369140625\n",
      "Loss 28369.603515625\n",
      "Loss 25286.2890625\n",
      "Loss 47614.2734375\n",
      "Loss 51661.8125\n",
      "Loss 9711.2138671875\n",
      "Loss 23804.669921875\n",
      "Loss 14193.0419921875\n",
      "Loss 24694.927734375\n",
      "Loss 41966.890625\n",
      "Loss 16493.142578125\n",
      "Loss 15321.48046875\n",
      "Loss 26255.41796875\n",
      "Loss 18396.38671875\n",
      "Loss 73149.9765625\n",
      "Loss 12514.720703125\n",
      "Loss 50451.88671875\n",
      "Loss 15151.8125\n",
      "Loss 12655.7626953125\n",
      "Loss 20246.404296875\n",
      "Loss 16241.8525390625\n",
      "Loss 24240.08203125\n",
      "Loss 16193.8232421875\n",
      "Loss 12845.5234375\n",
      "Loss 25403.853515625\n",
      "Loss 18028.70703125\n",
      "Loss 21211.07421875\n",
      "Loss 52838.52734375\n",
      "Loss 44778.70703125\n",
      "Loss 9325.255859375\n",
      "Loss 37725.59375\n",
      "Loss 13704.5546875\n",
      "Loss 35195.4453125\n",
      "Loss 25717.376953125\n",
      "Loss 73111.953125\n",
      "Loss 74338.65625\n",
      "Loss 21837.341796875\n",
      "Loss 32800.90234375\n",
      "Loss 53848.70703125\n",
      "Loss 57557.92578125\n",
      "Loss 11379.8212890625\n",
      "Loss 25487.068359375\n",
      "Loss 16933.599609375\n",
      "Loss 52305.53515625\n",
      "Loss 36628.375\n",
      "Loss 18001.42578125\n",
      "Loss 41457.2734375\n",
      "Loss 26777.80078125\n",
      "Loss 15358.8896484375\n",
      "Loss 79143.640625\n",
      "Loss 23526.916015625\n",
      "Loss 79927.9453125\n",
      "Loss 16079.8740234375\n",
      "Loss 31721.19921875\n",
      "Loss 9727.4658203125\n",
      "Loss 35822.171875\n",
      "Loss 17724.76953125\n",
      "Loss 16930.01171875\n",
      "Loss 19968.44140625\n",
      "Loss 39429.46484375\n",
      "Loss 51105.78125\n",
      "Loss 33166.14453125\n",
      "Loss 20553.521484375\n",
      "Loss 25737.642578125\n",
      "Loss 57541.484375\n",
      "Loss 40676.109375\n",
      "Loss 10966.2275390625\n",
      "Loss 41026.64453125\n",
      "Loss 53710.65625\n",
      "Loss 22674.005859375\n",
      "Loss 55027.24609375\n",
      "Loss 12703.44921875\n",
      "Loss 8351.2578125\n",
      "Loss 18654.494140625\n",
      "Loss 18005.13671875\n",
      "Loss 29223.287109375\n",
      "Loss 20686.4453125\n",
      "Loss 20096.61328125\n",
      "Loss 19369.982421875\n",
      "Loss 78365.078125\n",
      "Loss 36326.484375\n",
      "Loss 55029.109375\n",
      "Loss 22380.94921875\n",
      "Loss 47607.8125\n",
      "Loss 19581.8671875\n",
      "Loss 46467.31640625\n",
      "Loss 46952.54296875\n",
      "Loss 17293.44921875\n",
      "Loss 35668.0703125\n",
      "Loss 45199.265625\n",
      "Loss 25929.169921875\n",
      "Loss 12954.064453125\n",
      "Loss 33278.84375\n",
      "Loss 25485.96484375\n",
      "Loss 21825.693359375\n",
      "Loss 18818.9921875\n",
      "Loss 23601.71484375\n",
      "Loss 45235.37109375\n",
      "Loss 15154.2158203125\n",
      "Loss 15428.095703125\n",
      "Loss 43886.375\n",
      "Loss 19303.40625\n",
      "Loss 24103.83203125\n",
      "Loss 21310.55859375\n",
      "Loss 54680.5625\n",
      "Loss 30443.61328125\n",
      "Loss 16026.21484375\n",
      "Loss 26608.275390625\n",
      "Loss 12969.8818359375\n",
      "Loss 43834.40625\n",
      "Loss 24101.275390625\n",
      "Loss 15595.8623046875\n",
      "Loss 12042.48046875\n",
      "Loss 32554.109375\n",
      "Loss 45252.65234375\n",
      "Loss 44848.8203125\n",
      "Loss 47120.66015625\n",
      "Loss 106734.5859375\n",
      "Loss 30662.833984375\n",
      "Loss 20884.248046875\n",
      "Loss 28558.8515625\n",
      "Loss 11866.4189453125\n",
      "Loss 43695.37890625\n",
      "Loss 23213.6875\n",
      "Loss 20607.44921875\n",
      "Loss 45395.8515625\n",
      "Loss 47809.67578125\n",
      "Loss 17624.158203125\n",
      "Loss 29055.599609375\n",
      "Loss 18471.705078125\n",
      "Loss 23485.30859375\n",
      "Loss 32750.5234375\n",
      "Loss 69264.5\n",
      "Loss 18695.4140625\n",
      "Loss 20025.154296875\n",
      "Loss 21050.22265625\n",
      "Loss 20466.330078125\n",
      "Loss 25256.572265625\n",
      "Loss 15285.392578125\n",
      "Loss 23476.171875\n",
      "Loss 58990.34765625\n",
      "Loss 17250.4375\n",
      "Loss 28976.197265625\n",
      "Loss 16801.708984375\n",
      "Loss 61874.3359375\n",
      "Loss 20176.4609375\n",
      "Loss 28447.8828125\n",
      "Loss 39508.3984375\n",
      "Loss 18505.02734375\n",
      "Loss 37372.296875\n",
      "Loss 44839.25390625\n",
      "Loss 23656.177734375\n",
      "Loss 17162.83203125\n",
      "Loss 29263.234375\n",
      "Loss 16046.9970703125\n",
      "Loss 20398.54296875\n",
      "Loss 84754.6171875\n",
      "Loss 30511.45703125\n",
      "Loss 14636.50390625\n",
      "Loss 35258.1171875\n",
      "Loss 14347.74609375\n",
      "Loss 51703.5703125\n",
      "Loss 67399.96875\n",
      "Loss 11916.7119140625\n",
      "Loss 41985.17578125\n",
      "Loss 19587.029296875\n",
      "Loss 40075.234375\n",
      "Loss 23734.76171875\n",
      "Loss 25370.9375\n",
      "Loss 56266.8125\n",
      "Loss 21236.103515625\n",
      "Loss 34933.4609375\n",
      "Loss 32690.734375\n",
      "Loss 10993.6171875\n",
      "Loss 28930.884765625\n"
     ]
    }
   ],
   "source": [
    "# Task 4\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Neural Networks Model - Updated with more Layers\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialise the Parameters\n",
    "        self.layers = torch.nn.Sequential( # Update Model with more Layers\n",
    "        torch.nn.Linear(11, 20),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(20, 10),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Use the layers to process the features\n",
    "        return self.layers(features)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train function with Tensorboard\n",
    "def train(model, dataloader, epochs=15):\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        current_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction,labels)\n",
    "            loss.backward() \n",
    "            optimiser.step() # Optimiser step\n",
    "            optimiser.zero_grad()\n",
    "            ls = loss.item()\n",
    "            print(\"Loss\", ls)\n",
    "            batch_idx += 1\n",
    "            current_loss = current_loss + ls\n",
    "        \n",
    "        # print (f\"currentnt loss {current_loss} and batch index {batch_idx}\")\n",
    "        # print(f'Loss after mini-batch  ({epoch + 1} : {current_loss // batch_idx}')\n",
    "            writer.add_scalar('loss',current_loss / batch_idx , epoch)\n",
    "        \n",
    "train(model,train_loader)\n",
    "\n",
    "# The Plotting does not seem okay\n",
    "# Do we visualize the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquimbolosfernandez/miniforge3/envs/Tensorflow/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/joaquimbolosfernandez/miniforge3/envs/Tensorflow/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 18310.115234375\n",
      "Loss 22052.578125\n",
      "Loss 1733978752.0\n",
      "Loss 59853812.0\n",
      "Loss 16750.033203125\n",
      "Loss 15768.2998046875\n",
      "Loss 10666.501953125\n",
      "Loss 3181.186279296875\n",
      "Loss 6162.72802734375\n",
      "Loss 3274.758056640625\n",
      "Loss 6310.32373046875\n",
      "Loss 15342.03125\n",
      "Loss 15253.5\n",
      "Loss 26386.3984375\n",
      "Loss 10649.123046875\n",
      "Loss 31295.318359375\n",
      "Loss 25279.4453125\n",
      "Loss 5948.83251953125\n",
      "Loss 8617.21875\n",
      "Loss 7330.173828125\n",
      "Loss 39545.16015625\n",
      "Loss 22756.107421875\n",
      "Loss 4493.6162109375\n",
      "Loss 3573.541748046875\n",
      "Loss 26726.978515625\n",
      "Loss 5459.2275390625\n",
      "Loss 3902.8896484375\n",
      "Loss 11376.8916015625\n",
      "Loss 6706.4072265625\n",
      "Loss 17096.822265625\n",
      "Loss 11234.052734375\n",
      "Loss 35898.1640625\n",
      "Loss 23220.462890625\n",
      "Loss 14199.22265625\n",
      "Loss 4454.2490234375\n",
      "Loss 21123.3828125\n",
      "Loss 9465.3984375\n",
      "Loss 27822.3125\n",
      "Loss 8875.0322265625\n",
      "Loss 6949.73291015625\n",
      "Loss 12910.408203125\n",
      "Loss 7499.80126953125\n",
      "Loss 5606.97314453125\n",
      "Loss 2400.288330078125\n",
      "Loss 19259.853515625\n",
      "Loss 18634.17578125\n",
      "Loss 9307.646484375\n",
      "Loss 21439.77734375\n",
      "Loss 3400.17626953125\n",
      "Loss 14102.681640625\n",
      "Loss 10742.4560546875\n",
      "Loss 7624.328125\n",
      "Loss 21539.08984375\n",
      "Loss 2888.489501953125\n",
      "Loss 2667.201171875\n",
      "Loss 6426.3359375\n",
      "Loss 3693.3037109375\n",
      "Loss 63389.6796875\n",
      "Loss 14934.47265625\n",
      "Loss 7622.0576171875\n",
      "Loss 15998.888671875\n",
      "Loss 7030.52734375\n",
      "Loss 28558.525390625\n",
      "Loss 13048.4521484375\n",
      "Loss 19571.953125\n",
      "Loss 17056.16015625\n",
      "Loss 1057.101806640625\n",
      "Loss 8365.9521484375\n",
      "Loss 25514.44921875\n",
      "Loss 6636.345703125\n",
      "Loss 5949.80029296875\n",
      "Loss 5594.85888671875\n",
      "Loss 11391.208984375\n",
      "Loss 25127.90234375\n",
      "Loss 31618.388671875\n",
      "Loss 43854.65234375\n",
      "Loss 15549.82421875\n",
      "Loss 4380.51806640625\n",
      "Loss 39703.91796875\n",
      "Loss 18286.1875\n",
      "Loss 11081.099609375\n",
      "Loss 13651.484375\n",
      "Loss 31880.2890625\n",
      "Loss 11819.119140625\n",
      "Loss 35213.828125\n",
      "Loss 17993.833984375\n",
      "Loss 25345.58984375\n",
      "Loss 10201.1796875\n",
      "Loss 5210.3115234375\n",
      "Loss 7058.94970703125\n",
      "Loss 20980.06640625\n",
      "Loss 3452.14404296875\n",
      "Loss 5593.22314453125\n",
      "Loss 40204.91796875\n",
      "Loss 18080.443359375\n",
      "Loss 12446.708984375\n",
      "Loss 21688.03125\n",
      "Loss 10013.61328125\n",
      "Loss 6027.81103515625\n",
      "Loss 25120.017578125\n",
      "Loss 24305.93359375\n",
      "Loss 4512.884765625\n",
      "Loss 6210.82763671875\n",
      "Loss 2183.833251953125\n",
      "Loss 5457.7685546875\n",
      "Loss 7930.00634765625\n",
      "Loss 10333.1083984375\n",
      "Loss 5019.77392578125\n",
      "Loss 4587.96533203125\n",
      "Loss 11040.23828125\n",
      "Loss 14685.458984375\n",
      "Loss 13528.5478515625\n",
      "Loss 3136.766845703125\n",
      "Loss 16773.49609375\n",
      "Loss 5818.55615234375\n",
      "Loss 32348.4921875\n",
      "Loss 3854.697021484375\n",
      "Loss 3945.318359375\n",
      "Loss 5830.5048828125\n",
      "Loss 14227.783203125\n",
      "Loss 5239.3369140625\n",
      "Loss 12697.57421875\n",
      "Loss 10471.61328125\n",
      "Loss 9864.1591796875\n",
      "Loss 55861.9765625\n",
      "Loss 8960.2080078125\n",
      "Loss 34054.84765625\n",
      "Loss 5147.06005859375\n",
      "Loss 55617.96484375\n",
      "Loss 18553.97265625\n",
      "Loss 4825.001953125\n",
      "Loss 4515.34375\n",
      "Loss 4878.9423828125\n",
      "Loss 1757.887939453125\n",
      "Loss 4673.2255859375\n",
      "Loss 26373.509765625\n",
      "Loss 2753.966552734375\n",
      "Loss 7319.31005859375\n",
      "Loss 4933.171875\n",
      "Loss 1734.22314453125\n",
      "Loss 39319.796875\n",
      "Loss 12633.84765625\n",
      "Loss 6917.490234375\n",
      "Loss 14403.6689453125\n",
      "Loss 9950.53125\n",
      "Loss 13873.9111328125\n",
      "Loss 23800.580078125\n",
      "Loss 4275.4921875\n",
      "Loss 12270.8681640625\n",
      "Loss 15082.3974609375\n",
      "Loss 32816.81640625\n",
      "Loss 13075.0703125\n",
      "Loss 11515.0830078125\n",
      "Loss 13907.45703125\n",
      "Loss 6123.0087890625\n",
      "Loss 12337.8291015625\n",
      "Loss 36510.7109375\n",
      "Loss 12278.236328125\n",
      "Loss 13690.3515625\n",
      "Loss 15354.00390625\n",
      "Loss 10175.076171875\n",
      "Loss 16373.42578125\n",
      "Loss 29985.7265625\n",
      "Loss 8147.40380859375\n",
      "Loss 3927.59521484375\n",
      "Loss 2947.651611328125\n",
      "Loss 11351.09375\n",
      "Loss 10098.0869140625\n",
      "Loss 39556.73828125\n",
      "Loss 11586.1064453125\n",
      "Loss 3621.77392578125\n",
      "Loss 39864.70703125\n",
      "Loss 28189.283203125\n",
      "Loss 7998.2412109375\n",
      "Loss 23249.798828125\n",
      "Loss 6205.76220703125\n",
      "Loss 8892.59765625\n",
      "Loss 5742.83251953125\n",
      "Loss 20854.220703125\n",
      "Loss 5716.7001953125\n",
      "Loss 1425.7508544921875\n",
      "Loss 7811.32568359375\n",
      "Loss 7981.62060546875\n",
      "Loss 10792.1533203125\n",
      "Loss 10144.302734375\n",
      "Loss 30513.837890625\n",
      "Loss 14646.380859375\n",
      "Loss 8399.8291015625\n",
      "Loss 13836.28515625\n",
      "Loss 11698.66015625\n",
      "Loss 9043.4765625\n",
      "Loss 30249.9375\n",
      "Loss 8500.3349609375\n",
      "Loss 10273.2802734375\n",
      "Loss 4358.85986328125\n",
      "Loss 12717.2177734375\n",
      "Loss 22094.765625\n",
      "Loss 10404.6572265625\n",
      "Loss 16174.90234375\n",
      "Loss 3685.872802734375\n",
      "Loss 2574.906005859375\n",
      "Loss 25883.77734375\n",
      "Loss 16050.9765625\n",
      "Loss 33161.42578125\n",
      "Loss 11716.4521484375\n",
      "Loss 4360.056640625\n",
      "Loss 12498.3388671875\n",
      "Loss 7458.033203125\n",
      "Loss 7436.166015625\n",
      "Loss 10224.435546875\n",
      "Loss 7056.3681640625\n",
      "Loss 20078.888671875\n",
      "Loss 2321.6298828125\n",
      "Loss 14033.845703125\n",
      "Loss 8488.8642578125\n",
      "Loss 16805.4453125\n",
      "Loss 25353.375\n",
      "Loss 28354.6484375\n",
      "Loss 32739.576171875\n",
      "Loss 10594.384765625\n",
      "Loss 5701.71337890625\n",
      "Loss 6134.947265625\n",
      "Loss 44066.015625\n",
      "Loss 46936.28125\n",
      "Loss 7597.84375\n",
      "Loss 13696.6962890625\n",
      "Loss 4104.037109375\n",
      "Loss 5034.78369140625\n",
      "Loss 10329.884765625\n",
      "Loss 7542.7939453125\n",
      "Loss 4240.46337890625\n",
      "Loss 4921.00146484375\n",
      "Loss 10342.5810546875\n",
      "Loss 36186.7421875\n",
      "Loss 9217.5732421875\n",
      "Loss 7967.24365234375\n",
      "Loss 7439.70703125\n",
      "Loss 36171.04296875\n",
      "Loss 4331.8115234375\n",
      "Loss 40699.5625\n",
      "Loss 8958.4130859375\n",
      "Loss 1358.9677734375\n",
      "Loss 4791.8447265625\n",
      "Loss 15543.708984375\n",
      "Loss 3983.78125\n",
      "Loss 5641.19580078125\n",
      "Loss 29872.31640625\n",
      "Loss 16011.9345703125\n",
      "Loss 31486.126953125\n",
      "Loss 6980.5986328125\n",
      "Loss 17750.42578125\n",
      "Loss 32186.71484375\n",
      "Loss 6807.20068359375\n",
      "Loss 5602.353515625\n",
      "Loss 9191.232421875\n",
      "Loss 4682.78564453125\n",
      "Loss 37406.03515625\n",
      "Loss 8863.0703125\n",
      "Loss 25960.955078125\n",
      "Loss 12123.7890625\n",
      "Loss 9661.8857421875\n",
      "Loss 29932.265625\n",
      "Loss 11056.5712890625\n",
      "Loss 3176.646728515625\n",
      "Loss 2637.9404296875\n",
      "Loss 2033.191650390625\n",
      "Loss 5745.07177734375\n",
      "Loss 7061.63916015625\n",
      "Loss 27869.28125\n",
      "Loss 28691.255859375\n",
      "Loss 22349.75390625\n",
      "Loss 5210.30615234375\n",
      "Loss 3690.631103515625\n",
      "Loss 14754.9814453125\n",
      "Loss 6388.908203125\n",
      "Loss 3385.45947265625\n",
      "Loss 7345.98583984375\n",
      "Loss 12966.0625\n",
      "Loss 4669.94482421875\n",
      "Loss 38326.53515625\n",
      "Loss 19466.533203125\n",
      "Loss 14128.361328125\n",
      "Loss 7628.880859375\n",
      "Loss 3587.82275390625\n",
      "Loss 29884.7421875\n",
      "Loss 5603.23974609375\n",
      "Loss 5966.9453125\n",
      "Loss 32179.23046875\n",
      "Loss 12912.3916015625\n",
      "Loss 20443.607421875\n",
      "Loss 9100.447265625\n",
      "Loss 23195.173828125\n",
      "Loss 6076.802734375\n",
      "Loss 30452.865234375\n",
      "Loss 27937.59375\n",
      "Loss 35496.36328125\n",
      "Loss 14424.5302734375\n",
      "Loss 4740.0615234375\n",
      "Loss 4165.68505859375\n",
      "Loss 5122.255859375\n",
      "Loss 3159.513427734375\n",
      "Loss 4996.74658203125\n",
      "Loss 3652.28515625\n",
      "Loss 13833.693359375\n",
      "Loss 37450.4609375\n",
      "Loss 5040.34130859375\n",
      "Loss 8846.275390625\n",
      "Loss 8437.5185546875\n",
      "Loss 26305.908203125\n",
      "Loss 7115.96142578125\n",
      "Loss 9375.1162109375\n",
      "Loss 22668.763671875\n",
      "Loss 16111.787109375\n",
      "Loss 6647.7568359375\n",
      "Loss 15042.275390625\n",
      "Loss 7630.982421875\n",
      "Loss 27326.3515625\n",
      "Loss 10343.15234375\n",
      "Loss 1996.836669921875\n",
      "Loss 747.5034790039062\n",
      "Loss 31408.109375\n",
      "Loss 22158.6171875\n",
      "Loss 4673.1240234375\n",
      "Loss 3197.975341796875\n",
      "Loss 13042.51953125\n",
      "Loss 3771.389892578125\n",
      "Loss 32516.64453125\n",
      "Loss 12060.2705078125\n",
      "Loss 11173.4609375\n",
      "Loss 23708.26953125\n",
      "Loss 5370.98291015625\n",
      "Loss 32028.162109375\n",
      "Loss 6890.1396484375\n",
      "Loss 2707.41259765625\n",
      "Loss 2146.650146484375\n",
      "Loss 11283.1748046875\n",
      "Loss 5273.08154296875\n",
      "Loss 3431.404541015625\n",
      "Loss 10731.87109375\n",
      "Loss 6838.7421875\n",
      "Loss 4811.583984375\n",
      "Loss 22612.099609375\n",
      "Loss 13540.5673828125\n",
      "Loss 39411.75\n",
      "Loss 17627.2109375\n",
      "Loss 1553.61767578125\n",
      "Loss 33546.0703125\n",
      "Loss 22014.26953125\n",
      "Loss 26420.49609375\n",
      "Loss 11523.66015625\n",
      "Loss 41901.51953125\n",
      "Loss 7763.08740234375\n",
      "Loss 8136.97705078125\n",
      "Loss 12622.1142578125\n",
      "Loss 5514.8623046875\n",
      "Loss 6397.373046875\n",
      "Loss 4111.4091796875\n",
      "Loss 34838.0546875\n",
      "Loss 20114.064453125\n",
      "Loss 13928.6962890625\n",
      "Loss 28243.341796875\n",
      "Loss 30392.57421875\n",
      "Loss 17629.4765625\n",
      "Loss 10800.1806640625\n",
      "Loss 8097.666015625\n",
      "Loss 8172.828125\n",
      "Loss 8607.8759765625\n",
      "Loss 4508.1005859375\n",
      "Loss 9617.2763671875\n",
      "Loss 4807.1220703125\n",
      "Loss 23312.6171875\n",
      "Loss 7895.71240234375\n",
      "Loss 6686.5615234375\n",
      "Loss 2979.52587890625\n",
      "Loss 2066.873046875\n",
      "Loss 14501.384765625\n",
      "Loss 22200.1484375\n",
      "Loss 11259.1337890625\n",
      "Loss 6713.314453125\n",
      "Loss 23667.568359375\n",
      "Loss 6017.4130859375\n",
      "Loss 3205.248291015625\n",
      "Loss 14497.57421875\n",
      "Loss 3032.155029296875\n",
      "Loss 5811.46044921875\n",
      "Loss 2783.73291015625\n",
      "Loss 51676.85546875\n",
      "Loss 13772.412109375\n",
      "Loss 12699.5341796875\n",
      "Loss 56354.80859375\n",
      "Loss 19596.65234375\n",
      "Loss 13335.537109375\n",
      "Loss 5222.54443359375\n",
      "Loss 31311.78515625\n",
      "Loss 6076.15869140625\n",
      "Loss 15870.15234375\n",
      "Loss 2231.515869140625\n",
      "Loss 4399.82470703125\n",
      "Loss 14483.14453125\n",
      "Loss 6607.02587890625\n",
      "Loss 3597.7802734375\n",
      "Loss 27106.759765625\n",
      "Loss 5555.82470703125\n",
      "Loss 11335.884765625\n",
      "Loss 4435.83203125\n",
      "Loss 2880.199951171875\n",
      "Loss 46094.0078125\n",
      "Loss 8830.0029296875\n",
      "Loss 32922.11328125\n",
      "Loss 32186.41015625\n",
      "Loss 19885.9296875\n",
      "Loss 44138.0390625\n",
      "Loss 17313.720703125\n",
      "Loss 20506.220703125\n",
      "Loss 14787.79296875\n",
      "Loss 10534.22265625\n",
      "Loss 5101.8466796875\n",
      "Loss 10744.3037109375\n",
      "Loss 6335.2275390625\n",
      "Loss 3933.8369140625\n",
      "Loss 7834.12744140625\n",
      "Loss 11002.8427734375\n",
      "Loss 5672.42431640625\n",
      "Loss 2675.4833984375\n",
      "Loss 3651.363525390625\n",
      "Loss 44358.32421875\n",
      "Loss 12075.431640625\n",
      "Loss 9679.361328125\n",
      "Loss 23488.080078125\n",
      "Loss 15692.0419921875\n",
      "Loss 11153.59765625\n",
      "Loss 8483.28125\n",
      "Loss 7571.96484375\n",
      "Loss 4123.34912109375\n",
      "Loss 26118.060546875\n",
      "Loss 18089.5546875\n",
      "Loss 10500.9931640625\n",
      "Loss 50921.2109375\n",
      "Loss 34105.5546875\n",
      "Loss 12114.23046875\n",
      "Loss 4615.796875\n",
      "Loss 13425.76171875\n",
      "Loss 13712.689453125\n",
      "Loss 5119.73291015625\n",
      "Loss 15722.923828125\n",
      "Loss 6740.62890625\n",
      "Loss 12067.1083984375\n",
      "Loss 21641.462890625\n",
      "Loss 10164.341796875\n",
      "Loss 33577.18359375\n",
      "Loss 4821.70751953125\n",
      "Loss 22188.49609375\n",
      "Loss 4198.75537109375\n",
      "Loss 3453.3359375\n",
      "Loss 21516.287109375\n",
      "Loss 12024.8017578125\n",
      "Loss 5016.41357421875\n",
      "Loss 4394.2373046875\n",
      "Loss 4299.0263671875\n",
      "Loss 21288.37890625\n",
      "Loss 4469.8037109375\n",
      "Loss 6781.5546875\n",
      "Loss 11399.5078125\n",
      "Loss 7985.34716796875\n",
      "Loss 53244.11328125\n",
      "Loss 12807.5888671875\n",
      "Loss 6603.841796875\n",
      "Loss 10552.515625\n",
      "Loss 5719.5927734375\n",
      "Loss 24038.515625\n",
      "Loss 4254.7724609375\n",
      "Loss 11989.498046875\n",
      "Loss 28064.939453125\n",
      "Loss 10542.798828125\n",
      "Loss 7048.47314453125\n",
      "Loss 37096.765625\n",
      "Loss 19440.443359375\n",
      "Loss 10859.1279296875\n",
      "Loss 23994.7734375\n",
      "Loss 14147.220703125\n",
      "Loss 5078.8544921875\n",
      "Loss 9042.1689453125\n",
      "Loss 41162.3515625\n",
      "Loss 5460.9794921875\n",
      "Loss 5178.6103515625\n",
      "Loss 34830.359375\n",
      "Loss 21559.982421875\n",
      "Loss 6723.18310546875\n",
      "Loss 17944.259765625\n",
      "Loss 9996.5205078125\n",
      "Loss 4948.10693359375\n",
      "Loss 5366.06689453125\n",
      "Loss 1774.106201171875\n",
      "Loss 8447.1435546875\n",
      "Loss 9249.177734375\n",
      "Loss 10250.5087890625\n",
      "Loss 24014.98046875\n",
      "Loss 22368.345703125\n",
      "Loss 29588.62890625\n",
      "Loss 15180.26171875\n",
      "Loss 7330.66943359375\n",
      "Loss 1351.0880126953125\n",
      "Loss 15801.861328125\n",
      "Loss 10391.9794921875\n",
      "Loss 2244.906005859375\n",
      "Loss 5273.71826171875\n",
      "Loss 32243.529296875\n",
      "Loss 39301.37109375\n",
      "Loss 9117.421875\n",
      "Loss 5245.08837890625\n",
      "Loss 10043.3505859375\n",
      "Loss 6216.7900390625\n",
      "Loss 1987.0380859375\n",
      "Loss 13189.44921875\n",
      "Loss 3766.66259765625\n",
      "Loss 6439.71484375\n",
      "Loss 33825.8203125\n",
      "Loss 6538.59814453125\n",
      "Loss 29672.75\n",
      "Loss 3038.935791015625\n",
      "Loss 5102.740234375\n",
      "Loss 4995.9619140625\n",
      "Loss 50475.3515625\n",
      "Loss 43519.453125\n",
      "Loss 14620.8271484375\n",
      "Loss 9661.5322265625\n",
      "Loss 26646.455078125\n",
      "Loss 9640.0634765625\n",
      "Loss 15891.162109375\n",
      "Loss 4180.64306640625\n",
      "Loss 3530.009033203125\n",
      "Loss 5461.4287109375\n",
      "Loss 3300.978515625\n",
      "Loss 4738.24609375\n",
      "Loss 15083.6611328125\n",
      "Loss 16212.970703125\n",
      "Loss 7045.71484375\n",
      "Loss 30786.71875\n",
      "Loss 23699.578125\n",
      "Loss 21483.689453125\n",
      "Loss 8186.82275390625\n",
      "Loss 26615.455078125\n",
      "Loss 16699.427734375\n",
      "Loss 3844.1357421875\n",
      "Loss 4268.6669921875\n",
      "Loss 5257.93310546875\n",
      "Loss 3939.143310546875\n",
      "Loss 3645.943359375\n",
      "Loss 25188.2734375\n",
      "Loss 6525.5419921875\n",
      "Loss 31307.009765625\n",
      "Loss 14852.341796875\n",
      "Loss 13922.20703125\n",
      "Loss 6587.13427734375\n",
      "Loss 22666.38671875\n",
      "Loss 14066.6083984375\n",
      "Loss 16647.984375\n",
      "Loss 3725.270263671875\n",
      "Loss 11014.53125\n",
      "Loss 9814.5615234375\n",
      "Loss 4683.8837890625\n",
      "Loss 39910.078125\n",
      "Loss 6338.10546875\n",
      "Loss 10141.4951171875\n",
      "Loss 8515.23046875\n",
      "Loss 5133.140625\n",
      "Loss 51700.04296875\n",
      "Loss 7292.40380859375\n",
      "Loss 9080.375\n",
      "Loss 29528.640625\n",
      "Loss 9989.4326171875\n",
      "Loss 4451.732421875\n",
      "Loss 2589.896728515625\n",
      "Loss 33361.75390625\n",
      "Loss 40364.71484375\n",
      "Loss 13084.46875\n",
      "Loss 7775.923828125\n",
      "Loss 6965.240234375\n",
      "Loss 7854.83984375\n",
      "Loss 16269.7021484375\n",
      "Loss 30896.658203125\n",
      "Loss 11801.7412109375\n",
      "Loss 4033.519287109375\n",
      "Loss 4531.35546875\n",
      "Loss 11491.4130859375\n"
     ]
    }
   ],
   "source": [
    "# Task 5 \n",
    "\n",
    "# yaml file\n",
    "''' \n",
    "optimiser: SGD\n",
    "lr: 0.001\n",
    "hidden_layer_width: 32\n",
    "depth: 5\n",
    "'''\n",
    "# Define function get_nn_config()\n",
    "import yaml\n",
    "def get_nn_config():\n",
    "    with open('nn_config.yaml', 'r') as stream:\n",
    "    # Converts yaml document to python object\n",
    "        dictionary=yaml.safe_load(stream)\n",
    "    return dictionary\n",
    "\n",
    "# Retrieve config dictionary\n",
    "nn_config = get_nn_config()\n",
    "\n",
    "# Redefine NeuralNetwork to include the custom numbers of hidden layers (depth) and hidden layers width\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialise the Parameters\n",
    "        #self.linear_layer = torch.nn.Linear(11,1) # 11 features, 1 label\n",
    "\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module(\"Input Layer\", torch.nn.Linear(11, nn_config['hidden_layer_width'])) # Input layer\n",
    "        self.layers.add_module(\"ReLU\", torch.nn.ReLU())\n",
    "        for i in range(nn_config['depth'] - 2): #  The input and the first linear layer are already taken into account\n",
    "            self.layers.add_module(\"Hidden Layer\", torch.nn.Linear(nn_config['hidden_layer_width'], nn_config['hidden_layer_width'])) # Hidden Layer\n",
    "            self.layers.add_module(\"Hidden ReLU\", torch.nn.ReLU())\n",
    "        self.layers.add_module(\"Output Layer\", torch.nn.Linear(nn_config['hidden_layer_width'], 1))# output layer\n",
    "    \n",
    "\n",
    "    def forward(self, features):\n",
    "        # Use the layers to process the features\n",
    "        return self.layers(features)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Train function with config \n",
    "def train(model, dataloader, nn_config, epochs=15):\n",
    "\n",
    "    # Set optimiser with lr from nn_config\n",
    "    if nn_config['optimiser'] == \"SGD\":\n",
    "        optimiser = torch.optim.SGD(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "    elif nn_config['optimiser'] == \"Adam\":\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "    elif nn_config['optimiser'] == \"Adagrad\":\n",
    "        optimiser = torch.optim.Adagrad(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    batch_idx = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        current_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction,labels)\n",
    "            loss.backward() \n",
    "            optimiser.step() # Optimiser step\n",
    "            optimiser.zero_grad()\n",
    "            ls = loss.item()\n",
    "            print(\"Loss\", ls)\n",
    "            batch_idx += 1\n",
    "            current_loss = current_loss + ls\n",
    "        \n",
    "        # print (f\"currentnt loss {current_loss} and batch index {batch_idx}\")\n",
    "        # print(f'Loss after mini-batch  ({epoch + 1} : {current_loss // batch_idx}')\n",
    "            writer.add_scalar('loss',current_loss / batch_idx , epoch)\n",
    "\n",
    "train(model,train_loader,nn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 46977.3359375\n",
      "Loss 11844.857421875\n",
      "Loss 87655.125\n",
      "Loss 13715.0302734375\n",
      "Loss 94273416.0\n",
      "Loss 32588.1640625\n",
      "Loss 33342.234375\n",
      "Loss 14352.0283203125\n",
      "Loss 35434.6171875\n",
      "Loss 64561.578125\n",
      "Loss 27883.984375\n",
      "Loss 26150260736.0\n",
      "Loss 4.908231502803763e+16\n",
      "Loss inf\n",
      "Loss inf\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n"
     ]
    }
   ],
   "source": [
    "# Task 6\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "def save_model(best_model, best_hyperparameters, best_metrics):\n",
    "    '''\n",
    "        Creates a models folder, then within the models' folder creates a regression folder and finally creates a last neural networks folder where it stores the model, a dictionary of its hyperparameters and a dictionary of its metrics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_name: str\n",
    "            A string used to name the folder to be created\n",
    "        \n",
    "        best_model: pytorch model\n",
    "            A model from pythorch\n",
    "        \n",
    "        best_hyperparameters: dict\n",
    "            A dictionary containing the optimal hyperparameters configuration\n",
    "        \n",
    "        best_metrics: dict \n",
    "            A dictionary containing the test metrics obtained using the best model   \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None       \n",
    "    '''\n",
    "\n",
    "    # Create Models folder\n",
    "    models_dir = 'airbnb-property-listings/models'\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    models_path = os.path.join(current_dir, models_dir)\n",
    "    if os.path.exists(models_path) == False:\n",
    "        os.mkdir(models_path)\n",
    "\n",
    "    # Create regression folder\n",
    "    regression_dir = 'airbnb-property-listings/models/regression'\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    regression_path = os.path.join(current_dir, regression_dir)\n",
    "    if os.path.exists(regression_path) == False:\n",
    "        os.mkdir(regression_path)\n",
    "\n",
    "    # Create neural_networks folder\n",
    "    nn_name_dir = os.path.join(regression_path,'neural_networks') # Create the neural network folder\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    nn_name_path = os.path.join(current_dir, nn_name_dir)\n",
    "    if os.path.exists(nn_name_path) == False:\n",
    "        os.mkdir(nn_name_path)\n",
    "\n",
    "    # Create a Timestamp folder\n",
    "    timestamp_dir = os.path.join(nn_name_dir,time.strftime(\"%Y-%m-%d_%H:%M:%S\")) # Create the timestamp folder\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    timestamp_path = os.path.join(current_dir, timestamp_dir)\n",
    "    if os.path.exists(timestamp_path) == False:\n",
    "        os.mkdir(timestamp_path)\n",
    "\n",
    "    # Save the model in a file called model.pt\n",
    "    torch.save(best_model, os.path.join(timestamp_path, 'model.pt')) \n",
    "   \n",
    "    # Save the hyperparameters in a file called hyperparameters.json\n",
    "    with open(os.path.join(timestamp_path, 'hyperparameters.json'), 'w') as fp: \n",
    "            json.dump(best_hyperparameters, fp)\n",
    "\n",
    "    # Save the metrics in a file called metrics.json\n",
    "    with open(os.path.join(timestamp_path, 'metrics.json'), 'w') as fp:\n",
    "            json.dump(best_metrics, fp)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Define the model\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "best_model = train(model,train_loader,nn_config)\n",
    "\n",
    "# Define the hyperparemeters\n",
    "\n",
    "best_hyperparameters = get_nn_config()\n",
    "\n",
    "# Calculate the metrics\n",
    "\n",
    "''' \n",
    "The RMSE loss of your model under a key called RMSE_loss for training, validation, and test sets\n",
    "The R^2 score of your model under a key called R_squared for training, validation, and test sets\n",
    "The time taken to train the model under a key called training_duration\n",
    "The average time taken to make a prediction under a key called inference_latency\n",
    "\n",
    "'''\n",
    "best_metrics = {\n",
    "\n",
    "    'RMSE_loss' : 0, #[training, validation, test], # Need to Calculate the metrics\n",
    "    'R^2' : 0, #[training, validation, test],\n",
    "    'training_duration' : 0,\n",
    "    'inference_latency' : 0,\n",
    "}\n",
    "\n",
    "save_model(best_model, best_hyperparameters, best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b64a52f329bd7ac83fd7355bf526de5da9b8503da8fcdfb6f417a855f85dd2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
