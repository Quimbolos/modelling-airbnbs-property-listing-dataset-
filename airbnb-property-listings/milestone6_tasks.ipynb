{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the DataSet\n",
    "\n",
    "from tabular_data import load_airbnb\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_airbnb()\n",
    "X.drop(532, axis=0, inplace=True)\n",
    "y.drop(532, axis=0, inplace=True)\n",
    "X['guests'] = X['guests'].str.replace('\\'','').astype(np.float64)\n",
    "X['bedrooms'] = X['bedrooms'].str.replace('\\'','').astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2.0000, 1.0000, 1.0000, 5.0000, 5.0000, 4.8000, 5.0000, 5.0000, 4.8000,\n",
      "        8.0000, 1.0000], dtype=torch.float64), tensor(126))\n",
      "889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(623, 133, 133)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Task 1\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# DataSet Class\n",
    "class AirbnbNightlyPriceImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.X, self.y = X , y\n",
    "    # Not dependent on index\n",
    "    def __getitem__(self, index):\n",
    "        features = torch.tensor(self.X.iloc[index])\n",
    "        label = torch.tensor(self.y.iloc[index])\n",
    "        return (features, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "dataset = AirbnbNightlyPriceImageDataset()\n",
    "print(dataset[10])\n",
    "print(len(dataset))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Split the data \n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader=DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "features, label = next(iter(train_loader))\n",
    "\n",
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n",
      "8976.1923828125\n",
      "41681.20703125\n",
      "144.17495727539062\n",
      "1080.975830078125\n",
      "46574.7734375\n",
      "36606.34765625\n",
      "189930.9375\n",
      "9511.267578125\n",
      "165432.3125\n",
      "142303.0625\n",
      "8160.90283203125\n",
      "28981.203125\n",
      "25553.189453125\n",
      "25264.111328125\n",
      "58445.6015625\n",
      "15704.583984375\n",
      "4660.96630859375\n",
      "100728.015625\n",
      "11439.498046875\n",
      "6745.86279296875\n",
      "138074.40625\n",
      "5712.9130859375\n",
      "33724.27734375\n",
      "37384.95703125\n",
      "75917.65625\n",
      "6645.6953125\n",
      "1690.5792236328125\n",
      "28790.171875\n",
      "14152.037109375\n",
      "10111.798828125\n",
      "8197.2958984375\n",
      "36691.36328125\n",
      "39970.93359375\n",
      "5218.03515625\n",
      "9230.552734375\n",
      "191564.46875\n",
      "13009.775390625\n",
      "87863.5234375\n",
      "1968.2587890625\n",
      "16388.71875\n",
      "20189.626953125\n",
      "31201.05859375\n",
      "15193.8759765625\n",
      "15467.3564453125\n",
      "4577.37353515625\n",
      "5488.25341796875\n",
      "11573.484375\n",
      "17792.314453125\n",
      "18158.97265625\n",
      "1293882.375\n",
      "23547.060546875\n",
      "16719.0859375\n",
      "60729.82421875\n",
      "305.7115173339844\n",
      "4744.9384765625\n",
      "15762.4560546875\n",
      "5627.78369140625\n",
      "66758.0\n",
      "8378.21875\n",
      "11769.4951171875\n",
      "22177.70703125\n",
      "6933.27880859375\n",
      "15309.0556640625\n",
      "1366.8486328125\n",
      "19752.498046875\n",
      "5868.55908203125\n",
      "24198.138671875\n",
      "2228.659423828125\n",
      "782.001220703125\n",
      "53933.32421875\n",
      "46833.47265625\n",
      "15661.302734375\n",
      "252977.625\n",
      "24170.716796875\n",
      "20857.6640625\n",
      "20167.865234375\n",
      "19507.6484375\n",
      "36954.15625\n",
      "5491.13671875\n",
      "50245.18359375\n",
      "1233.406005859375\n",
      "12749.24609375\n",
      "26589.87109375\n",
      "3316.54345703125\n",
      "2457.5615234375\n",
      "9737.9111328125\n",
      "189265.859375\n",
      "15159.1103515625\n",
      "13130.501953125\n",
      "6242.9375\n",
      "8069.46875\n",
      "25464.568359375\n",
      "1974.4703369140625\n",
      "24818.017578125\n",
      "8189.6962890625\n",
      "8371.541015625\n",
      "20007.876953125\n",
      "11296.216796875\n",
      "1458.864990234375\n",
      "3503.679931640625\n",
      "23255.93359375\n",
      "5100.4697265625\n",
      "107067.7578125\n",
      "6985.5556640625\n",
      "4153.8935546875\n",
      "13713.494140625\n",
      "15543.083984375\n",
      "13946.2578125\n",
      "7461.93505859375\n",
      "59558.68359375\n",
      "57974.70703125\n",
      "3452.7978515625\n",
      "23349.005859375\n",
      "62723.13671875\n",
      "11491.3515625\n",
      "1328.41796875\n",
      "12575.591796875\n",
      "8652.9765625\n",
      "16545.984375\n",
      "27278.537109375\n",
      "6823.623046875\n",
      "463561.84375\n",
      "19290.51171875\n",
      "8482.4248046875\n",
      "4894.3330078125\n",
      "353285.0625\n",
      "3820.984130859375\n",
      "11688.189453125\n",
      "33206.453125\n",
      "44033.87890625\n",
      "24631.4375\n",
      "20129.447265625\n",
      "8907.7578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/r5th0fxd26341bbn6qkwvlww0000gn/T/ipykernel_18972/1331631415.py:25: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(prediction, labels)\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "\n",
    "# Linear Model\n",
    "class LinearRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialise the Parameters\n",
    "        self.linear_layer = torch.nn.Linear(11,1) # 11 features, 1 label\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Use the layers to process the features\n",
    "        return self.linear_layer(features)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train function\n",
    "def train(model, dataloader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = F.mse_loss(prediction, labels)\n",
    "            loss.backward()\n",
    "            print(loss.item())\n",
    "            \n",
    "    return\n",
    "\n",
    "train(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquimbolosfernandez/miniforge3/envs/Tensorflow/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/joaquimbolosfernandez/miniforge3/envs/Tensorflow/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846.064453125\n",
      "5568.08984375\n",
      "32218.1015625\n",
      "9180.0224609375\n",
      "22692.2734375\n",
      "35139.41796875\n",
      "30036.26171875\n",
      "8397.1875\n",
      "16225.44921875\n",
      "5025.37109375\n",
      "13364.57421875\n",
      "24371.115234375\n",
      "5245.7021484375\n",
      "3320.156494140625\n",
      "42973.484375\n",
      "6852.39501953125\n",
      "5323.072265625\n",
      "6540.71728515625\n",
      "6123.32958984375\n",
      "5283.3291015625\n",
      "29783.53125\n",
      "5297.20947265625\n",
      "13820.828125\n",
      "9111.0693359375\n",
      "39330.5625\n",
      "28421.916015625\n",
      "8168.74951171875\n",
      "8301.4892578125\n",
      "4798.0419921875\n",
      "5464.75244140625\n",
      "6723.8271484375\n",
      "16827.951171875\n",
      "7520.798828125\n",
      "9887.7626953125\n",
      "2329.098388671875\n",
      "3342.658935546875\n",
      "4518.271484375\n",
      "2227.049560546875\n",
      "10585.759765625\n",
      "4580.3095703125\n",
      "1735.8590087890625\n",
      "3961.9833984375\n",
      "4267.80322265625\n",
      "2776.515869140625\n",
      "16631.537109375\n",
      "8746.634765625\n",
      "4288.75048828125\n",
      "7566.67236328125\n",
      "36799.6015625\n",
      "9039.3701171875\n",
      "16539.017578125\n",
      "5241.47998046875\n",
      "19978.546875\n",
      "5470.1650390625\n",
      "2831.311767578125\n",
      "12642.9228515625\n",
      "22870.111328125\n",
      "23769.658203125\n",
      "2752.163818359375\n",
      "24665.65625\n",
      "15770.7958984375\n",
      "3930.99072265625\n",
      "17506.7578125\n",
      "15089.2841796875\n",
      "35469.234375\n",
      "3668.16259765625\n",
      "6033.28076171875\n",
      "6066.61572265625\n",
      "7444.1328125\n",
      "34722.484375\n",
      "12210.7744140625\n",
      "3945.013916015625\n",
      "16831.28125\n",
      "7101.1708984375\n",
      "36981.51171875\n",
      "9117.55859375\n",
      "7772.49169921875\n",
      "3226.6337890625\n",
      "16775.53125\n",
      "5921.58203125\n",
      "5607.6953125\n",
      "11017.564453125\n",
      "38362.93359375\n",
      "11800.5908203125\n",
      "2390.265625\n",
      "16039.66796875\n",
      "3603.23876953125\n",
      "4541.017578125\n",
      "4446.91455078125\n",
      "13257.6982421875\n",
      "5401.2099609375\n",
      "17542.50390625\n",
      "7308.486328125\n",
      "12317.736328125\n",
      "9278.7919921875\n",
      "4515.28076171875\n",
      "26891.015625\n",
      "13445.09765625\n",
      "11307.6640625\n",
      "36231.77734375\n"
     ]
    }
   ],
   "source": [
    "# Task 3 \n",
    "\n",
    "model = LinearRegression()\n",
    "loss_fn = torch.nn.MSELoss() # This Loss function is better\n",
    "\n",
    "# Train function with optimiser\n",
    "def train(model, dataloader, epochs=100):\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction, labels)\n",
    "            loss.backward() # What does this do? Populates the gradients?\n",
    "            optimiser.step() # Optimiser step\n",
    "            optimiser.zero_grad()\n",
    "        print(loss.item())   \n",
    "    return\n",
    "\n",
    "train(model,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 18883.622314453125\n",
      "Loss 14252.867807241586\n",
      "Loss 13720.902005709135\n",
      "Loss 13595.596905048076\n",
      "Loss 13638.107553335336\n",
      "Loss 13656.456214317908\n",
      "Loss 13469.92507386819\n",
      "Loss 13545.59088291266\n",
      "Loss 13731.53598883213\n",
      "Loss 13769.813038361379\n",
      "Loss 13649.937318459535\n",
      "Loss 13513.16968399439\n",
      "Loss 13622.061172876603\n",
      "Loss 13554.998904497195\n",
      "Loss 13568.152087089344\n",
      "Loss 13661.958408453525\n",
      "Loss 13621.658065404647\n",
      "Loss 13620.61389473157\n",
      "Loss 13538.938254331932\n",
      "Loss 13760.293150290465\n"
     ]
    }
   ],
   "source": [
    "# Task 4\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# Neural Networks Model - Updated with more Layers\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialise the Parameters\n",
    "        self.layers = torch.nn.Sequential( # Update Model with more Layers\n",
    "        torch.nn.Linear(11, 512),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(512, 256),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Use the layers to process the features\n",
    "        return self.layers(features)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Train function with Tensorboard\n",
    "def train(model, dataloader, epochs=20):\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        current_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction,labels)\n",
    "            loss.backward() \n",
    "            optimiser.step() # Optimiser step\n",
    "            optimiser.zero_grad()\n",
    "            ls = loss.item()\n",
    "            batch_idx += 1\n",
    "            current_loss = current_loss + ls\n",
    "            # writer.add_scalar(\"Loss - Task 4\",ls, epoch)\n",
    "        \n",
    "        # print (f\"currentnt loss {current_loss} and batch index {batch_idx}\")\n",
    "        # print(f'Loss after mini-batch  ({epoch + 1} : {current_loss // batch_idx}')\n",
    "        writer.add_scalar('loss',current_loss / batch_idx , epoch)\n",
    "        print(\"Loss\", current_loss / batch_idx)\n",
    "            \n",
    "        \n",
    "train(model,train_loader)\n",
    "\n",
    "# The Plotting does not seem okay\n",
    "# Do we visualize the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 25287.65234375\n",
      "Loss 32571.419921875\n",
      "Loss 184456896.0\n",
      "Loss 38007.43359375\n",
      "Loss 1116664758272.0\n",
      "Loss 4792146.0\n",
      "Loss 4797858.0\n",
      "Loss 4038146.25\n",
      "Loss 7114414.5\n",
      "Loss 4686036.0\n",
      "Loss 4604442.0\n",
      "Loss 4813102.0\n",
      "Loss 4839096.5\n",
      "Loss 4513332.0\n",
      "Loss 4675208.0\n",
      "Loss 4933570.0\n",
      "Loss 4901843.0\n",
      "Loss 4764189.0\n",
      "Loss 4529743.0\n",
      "Loss 4559428.5\n",
      "Loss 4611916.5\n",
      "Loss 4440800.5\n",
      "Loss 4452646.5\n",
      "Loss 4380575.0\n",
      "Loss 4492293.0\n",
      "Loss 4428891.0\n",
      "Loss 4324718.0\n",
      "Loss 4525574.5\n",
      "Loss 4354404.0\n",
      "Loss 4382444.0\n",
      "Loss 4297299.0\n",
      "Loss 4310760.0\n",
      "Loss 27395100770304.0\n",
      "Loss inf\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n",
      "Loss nan\n"
     ]
    }
   ],
   "source": [
    "# Task 5 \n",
    "\n",
    "torch.manual_seed(10) \n",
    "\n",
    "# yaml file content\n",
    "''' \n",
    "optimiser: SGD\n",
    "lr: 0.001\n",
    "hidden_layer_width: 32\n",
    "depth: 5\n",
    "'''\n",
    "# Define function get_nn_config()\n",
    "import yaml\n",
    "def get_nn_config():\n",
    "    with open('nn_config.yaml', 'r') as stream:\n",
    "    # Converts yaml document to python object\n",
    "        dictionary=yaml.safe_load(stream)\n",
    "    return dictionary\n",
    "\n",
    "# Retrieve config dictionary\n",
    "nn_config = get_nn_config()\n",
    "\n",
    "# Redefine NeuralNetwork to include the custom numbers of hidden layers (depth) and hidden layers width\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialise the Parameters\n",
    "        #self.linear_layer = torch.nn.Linear(11,1) # 11 features, 1 label\n",
    "\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module(\"Input Layer\", torch.nn.Linear(11, nn_config['hidden_layer_width'])) # Input layer\n",
    "        self.layers.add_module(\"ReLU\", torch.nn.ReLU())\n",
    "        for i in range(nn_config['depth'] - 2): #  The input and the first linear layer are already taken into account\n",
    "            self.layers.add_module(\"Hidden Layer\", torch.nn.Linear(nn_config['hidden_layer_width'], nn_config['hidden_layer_width'])) # Hidden Layer\n",
    "            self.layers.add_module(\"Hidden ReLU\", torch.nn.ReLU())\n",
    "        self.layers.add_module(\"Output Layer\", torch.nn.Linear(nn_config['hidden_layer_width'], 1))# output layer\n",
    "    \n",
    "\n",
    "    def forward(self, features):\n",
    "        # Use the layers to process the features\n",
    "        return self.layers(features)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Train function with config \n",
    "def train(model, dataloader, nn_config, epochs=15):\n",
    "\n",
    "    # Set optimiser with lr from nn_config\n",
    "    if nn_config['optimiser'] == \"SGD\":\n",
    "        optimiser = torch.optim.SGD(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "    elif nn_config['optimiser'] == \"Adam\":\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "    elif nn_config['optimiser'] == \"Adagrad\":\n",
    "        optimiser = torch.optim.Adagrad(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    batch_idx = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        current_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction,labels)\n",
    "            loss.backward() \n",
    "            optimiser.step() # Optimiser step\n",
    "            optimiser.zero_grad()\n",
    "            ls = loss.item()\n",
    "            print(\"Loss\", ls)\n",
    "            batch_idx += 1\n",
    "            current_loss = current_loss + ls\n",
    "        writer.add_scalar('loss - Task 5',current_loss / batch_idx , epoch)\n",
    "        for batch in val_dataloader:\n",
    "            features, labels = batch\n",
    "            features = features.to(torch.float32) # Convert torch into the right format\n",
    "            labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "            prediction = model(features)\n",
    "            loss = loss_fn(prediction,labels)\n",
    "            #loss.backward() \n",
    "            #optimiser.step() # Optimiser step\n",
    "            #optimiser.zero_grad()\n",
    "            ls = loss.item()\n",
    "            print(\"Loss\", ls)\n",
    "            batch_idx += 1\n",
    "            current_loss = current_loss + ls\n",
    "        # print (f\"currentnt loss {current_loss} and batch index {batch_idx}\")\n",
    "        # print(f'Loss after mini-batch  ({epoch + 1} : {current_loss // batch_idx}')\n",
    "            writer.add_scalar('loss - Task 5',current_loss / batch_idx , epoch)\n",
    "\n",
    "train(model,train_loader,nn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 17071.94921875\n",
      "Loss 96078.3671875\n",
      "Loss 15029968896.0\n",
      "Loss 131251.5\n",
      "Loss 159014.4375\n",
      "Loss 201944.171875\n",
      "Loss 121386.9765625\n",
      "Loss 152356.1875\n",
      "Loss 2328529404428288.0\n",
      "Loss 9.941703480716933e+29\n",
      "Loss 3.976681929732662e+24\n",
      "Loss 3.9607906361739133e+24\n",
      "Loss 3.9449630415282944e+24\n",
      "Loss 3.9291982811046765e+24\n",
      "Loss 3.9134977960549405e+24\n",
      "Loss 3.897859568766453e+24\n",
      "Loss 3.8822835992392147e+24\n",
      "Loss 3.866769887473225e+24\n",
      "Loss 3.851318433468484e+24\n",
      "Loss 3.8359289489946156e+24\n",
      "Loss 3.8206008575908676e+24\n",
      "Loss 3.805333871026864e+24\n",
      "Loss 3.790127701072228e+24\n",
      "Loss 3.7749829241877123e+24\n",
      "Loss 3.7598980992214364e+24\n",
      "Loss 3.7448732261734e+24\n",
      "Loss 3.729908593273979e+24\n",
      "Loss 3.715003912292798e+24\n",
      "Loss 3.7001588949994803e+24\n",
      "Loss 3.68537325316365e+24\n",
      "Loss 3.670646122094178e+24\n",
      "Loss 3.6559783664821936e+24\n",
      "Loss 3.641369121636568e+24\n",
      "Loss 3.626818675787677e+24\n",
      "Loss 3.6123255877836407e+24\n",
      "Loss 3.5978901458548345e+24\n",
      "Loss 3.583513214692387e+24\n",
      "Loss 3.56919392960517e+24\n",
      "Loss 3.554931714132431e+24\n",
      "Loss 3.540725991813418e+24\n",
      "Loss 3.5265773391088826e+24\n",
      "Loss 3.512484891327697e+24\n",
      "Loss 3.498449224930613e+24\n",
      "Loss 3.484469763456879e+24\n",
      "Loss 3.47054535398499e+24\n",
      "Loss 3.4566777258972025e+24\n",
      "Loss 3.4428654380416364e+24\n",
      "Loss 3.4291070492664105e+24\n",
      "Loss 3.415404288953782e+24\n",
      "Loss 3.4017568688733746e+24\n",
      "Loss 3.38816392433406e+24\n",
      "Loss 3.374624014183957e+24\n",
      "Loss 3.3611394442660754e+24\n",
      "Loss 3.3477079087374056e+24\n",
      "Loss 3.3343299840587e+24\n",
      "Loss 3.321006246690711e+24\n",
      "Loss 3.3077343907904293e+24\n",
      "Loss 3.2945172986616164e+24\n",
      "Loss 3.281352664461263e+24\n",
      "Loss 3.268239911728617e+24\n",
      "Loss 3.2551804816155593e+24\n",
      "Loss 3.2421723565094564e+24\n",
      "Loss 3.2292175540229415e+24\n",
      "Loss 3.216313191852253e+24\n",
      "Loss 3.2034607111492723e+24\n",
      "Loss 3.1906595354532463e+24\n",
      "Loss 3.1779096647641753e+24\n",
      "Loss 3.165210810851683e+24\n",
      "Loss 3.152562685485394e+24\n",
      "Loss 3.139965288665307e+24\n",
      "Loss 3.1274174674699185e+24\n",
      "Loss 3.114920951281485e+24\n",
      "Loss 3.102473722487373e+24\n",
      "Loss 3.0900760693179596e+24\n",
      "Loss 3.0777282800036203e+24\n",
      "Loss 3.065429489853227e+24\n",
      "Loss 3.0531802753275313e+24\n",
      "Loss 3.0409797717354055e+24\n",
      "Loss 3.028827690846473e+24\n",
      "Loss 3.0167243208911105e+24\n",
      "Loss 3.0046693736389413e+24\n",
      "Loss 2.9926625608595894e+24\n",
      "Loss 2.980703594322679e+24\n",
      "Loss 2.9687927622585854e+24\n",
      "Loss 2.9569297764369333e+24\n",
      "Loss 2.9451134839362177e+24\n",
      "Loss 2.933344749447567e+24\n",
      "Loss 2.9216229965102293e+24\n",
      "Loss 2.9099485133545804e+24\n",
      "Loss 2.898320435289492e+24\n",
      "Loss 2.88673905054534e+24\n",
      "Loss 2.8752037826613723e+24\n",
      "Loss 2.863714343407213e+24\n",
      "Loss 2.8522710210132375e+24\n",
      "Loss 2.840873239018694e+24\n",
      "Loss 2.829520997423583e+24\n",
      "Loss 2.8182142962279036e+24\n",
      "Loss 2.806952558970904e+24\n",
      "Loss 2.795735785652584e+24\n",
      "Loss 2.7845639762729435e+24\n",
      "Loss 2.773437130831983e+24\n",
      "Loss 2.7623546728689495e+24\n",
      "Loss 2.7513160259230913e+24\n",
      "Loss 2.7403217664551604e+24\n",
      "Loss 2.7293713180044046e+24\n",
      "Loss 2.718464680570824e+24\n",
      "Loss 2.707601854154418e+24\n",
      "Loss 2.696782550524811e+24\n",
      "Loss 2.686006193221251e+24\n",
      "Loss 2.6752727822437375e+24\n",
      "Loss 2.6645823175922705e+24\n",
      "Loss 2.653934511036474e+24\n",
      "Loss 2.643329362576348e+24\n",
      "Loss 2.632766583981516e+24\n",
      "Loss 2.6222458870216025e+24\n",
      "Loss 2.611767559926983e+24\n",
      "Loss 2.6013307380065296e+24\n",
      "Loss 2.5909359977209943e+24\n",
      "Loss 2.580582762609625e+24\n",
      "Loss 2.570270456211669e+24\n",
      "Loss 2.559999943218255e+24\n",
      "Loss 2.5497700707078784e+24\n",
      "Loss 2.5395814151412915e+24\n",
      "Loss 2.529433400057742e+24\n",
      "Loss 2.5193251607661015e+24\n",
      "Loss 2.5092578501878745e+24\n",
      "Loss 2.499231468323061e+24\n",
      "Loss 2.489243997559028e+24\n",
      "Loss 2.4792974555084087e+24\n",
      "Loss 2.46938982455857e+24\n",
      "Loss 2.459522545861392e+24\n",
      "Loss 2.449694178264995e+24\n",
      "Loss 2.4399050099997543e+24\n",
      "Loss 2.4301553292960465e+24\n",
      "Loss 2.420444271462743e+24\n",
      "Loss 2.4107725570757842e+24\n",
      "Loss 2.4011387449832894e+24\n",
      "Loss 2.3915441322219512e+24\n",
      "Loss 2.381987565870265e+24\n",
      "Loss 2.372469045928231e+24\n",
      "Loss 2.3629885723958489e+24\n",
      "Loss 2.3535461452731188e+24\n",
      "Loss 2.3441408998689123e+24\n",
      "Loss 2.33477427733511e+24\n",
      "Loss 2.3254448365198315e+24\n",
      "Loss 2.3161522891927003e+24\n",
      "Loss 2.3068966353537165e+24\n",
      "Loss 2.2976781632332563e+24\n",
      "Loss 2.2884971610616959e+24\n",
      "Loss 2.2793521876871544e+24\n",
      "Loss 2.2702432431096318e+24\n",
      "Loss 2.261171768481009e+24\n",
      "Loss 2.252136322649405e+24\n",
      "Loss 2.243136617384444e+24\n",
      "Loss 2.234172940916502e+24\n",
      "Loss 2.2252451491303908e+24\n",
      "Loss 2.2163530979109224e+24\n",
      "Loss 2.2074966431429088e+24\n",
      "Loss 2.1986754965959737e+24\n",
      "Loss 2.189889225924553e+24\n",
      "Loss 2.181138551704587e+24\n",
      "Loss 2.1724233298208877e+24\n",
      "Loss 2.1637421191215743e+24\n",
      "Loss 2.1550960725281514e+24\n",
      "Loss 2.1464840371191144e+24\n",
      "Loss 2.137907165815968e+24\n",
      "Loss 2.1293640174668312e+24\n",
      "Loss 2.1208545920717042e+24\n",
      "Loss 2.1123803307824677e+24\n",
      "Loss 2.1039389277561126e+24\n",
      "Loss 2.0955315359141433e+24\n",
      "Loss 2.0871578670261837e+24\n",
      "Loss 2.0788176328618578e+24\n",
      "Loss 2.0705108334211655e+24\n",
      "Loss 2.0622366040129783e+24\n",
      "Loss 2.053996385789177e+24\n",
      "Loss 2.0457887375978809e+24\n",
      "Loss 2.0376133712087137e+24\n",
      "Loss 2.0294714395431802e+24\n",
      "Loss 2.0213609249886471e+24\n",
      "Loss 2.0132844216185e+24\n",
      "Loss 2.005238470668225e+24\n",
      "Loss 1.9972256662112074e+24\n",
      "Loss 1.9892451435563188e+24\n",
      "Loss 1.9812957497820546e+24\n",
      "Loss 1.9733786378099194e+24\n",
      "Loss 1.965493519409537e+24\n",
      "Loss 1.9576389534290267e+24\n",
      "Loss 1.9498163810202693e+24\n",
      "Loss 1.9420249374921362e+24\n",
      "Loss 1.9342646228446275e+24\n",
      "Loss 1.9265354370777432e+24\n",
      "Loss 1.918837091961107e+24\n",
      "Loss 1.9111692992643431e+24\n",
      "Loss 1.9035322031026393e+24\n",
      "Loss 1.8959253711304314e+24\n",
      "Loss 1.8883495239236598e+24\n",
      "Loss 1.880803652676008e+24\n",
      "Loss 1.8732886220786044e+24\n",
      "Loss 1.8658027027491922e+24\n",
      "Loss 1.8583469034940878e+24\n",
      "Loss 1.850920936082915e+24\n",
      "Loss 1.843524800515674e+24\n",
      "Loss 1.8361579203316125e+24\n",
      "Loss 1.8288207278762945e+24\n",
      "Loss 1.8215132231497201e+24\n",
      "Loss 1.8142339650000087e+24\n",
      "Loss 1.8069842504638527e+24\n",
      "Loss 1.799763647195688e+24\n",
      "Loss 1.7925717228499506e+24\n",
      "Loss 1.7854086215418283e+24\n",
      "Loss 1.778274055040945e+24\n",
      "Loss 1.7711681674624887e+24\n",
      "Loss 1.7640908146912715e+24\n",
      "Loss 1.7570409879209768e+24\n",
      "Loss 1.750019695957921e+24\n",
      "Loss 1.7430269388021044e+24\n",
      "Loss 1.7360617076472102e+24\n",
      "Loss 1.7291241466084266e+24\n",
      "Loss 1.7222146880313178e+24\n",
      "Loss 1.7153326113399434e+24\n",
      "Loss 1.7084780606494914e+24\n",
      "Loss 1.701650891844774e+24\n",
      "Loss 1.6948509608106027e+24\n",
      "Loss 1.688078267546978e+24\n",
      "Loss 1.6813329561690874e+24\n",
      "Loss 1.674614017870615e+24\n",
      "Loss 1.6679224614578767e+24\n",
      "Loss 1.6612574222397446e+24\n",
      "Loss 1.6546189002162183e+24\n",
      "Loss 1.648006895387298e+24\n",
      "Loss 1.6414222724441122e+24\n",
      "Loss 1.6348628696588396e+24\n",
      "Loss 1.628329839952985e+24\n",
      "Loss 1.6218233274417362e+24\n",
      "Loss 1.615342323318777e+24\n",
      "Loss 1.6088876922752354e+24\n",
      "Loss 1.6024584255047954e+24\n",
      "Loss 1.5960549553530209e+24\n",
      "Loss 1.5896771377047239e+24\n",
      "Loss 1.5833248284447163e+24\n",
      "Loss 1.57699788345781e+24\n",
      "Loss 1.5706963027440052e+24\n",
      "Loss 1.5644197980729255e+24\n",
      "Loss 1.558168369444571e+24\n",
      "Loss 1.5519420168589418e+24\n",
      "Loss 1.5457404520856615e+24\n",
      "Loss 1.5395635310095423e+24\n",
      "Loss 1.53341154186096e+24\n",
      "Loss 1.527284196409539e+24\n",
      "Loss 1.5211809181945265e+24\n",
      "Loss 1.515102427791863e+24\n",
      "Loss 1.5090480046256082e+24\n",
      "Loss 1.5030177928109502e+24\n",
      "Loss 1.497011936463077e+24\n",
      "Loss 1.49102957089086e+24\n",
      "Loss 1.48507141667024e+24\n",
      "Loss 1.4791371855708405e+24\n",
      "Loss 1.4732263011319093e+24\n",
      "Loss 1.4673393398141986e+24\n",
      "Loss 1.4614755810417682e+24\n",
      "Loss 1.4556354571601822e+24\n",
      "Loss 1.4498185358238765e+24\n",
      "Loss 1.444025105263227e+24\n",
      "Loss 1.4382547331326698e+24\n",
      "Loss 1.4325077076625808e+24\n",
      "Loss 1.4267831641618317e+24\n",
      "Loss 1.4210819673215508e+24\n",
      "Loss 1.415403540680986e+24\n",
      "Loss 1.409747596009761e+24\n",
      "Loss 1.4041142774230638e+24\n",
      "Loss 1.3985034408057065e+24\n",
      "Loss 1.392914942042501e+24\n",
      "Loss 1.387348637018259e+24\n",
      "Loss 1.381804813963357e+24\n",
      "Loss 1.3762828964170426e+24\n",
      "Loss 1.3707833167248798e+24\n",
      "Loss 1.3653056425413046e+24\n",
      "Loss 1.359850017981505e+24\n",
      "Loss 1.3544160106999168e+24\n",
      "Loss 1.34900362069654e+24\n",
      "Loss 1.3436129920865626e+24\n",
      "Loss 1.3382439807547965e+24\n",
      "Loss 1.3328961543556777e+24\n",
      "Loss 1.3275702334651464e+24\n",
      "Loss 1.322265065161698e+24\n",
      "Loss 1.316981514136461e+24\n",
      "Loss 1.311719003928683e+24\n",
      "Loss 1.306477246307988e+24\n",
      "Loss 1.301256529504752e+24\n",
      "Loss 1.296056709403787e+24\n",
      "Loss 1.290877786005093e+24\n",
      "Loss 1.285719471078294e+24\n",
      "Loss 1.2805814763930135e+24\n",
      "Loss 1.2754643784100041e+24\n",
      "Loss 1.2703674565533253e+24\n",
      "Loss 1.2652911431685414e+24\n",
      "Loss 1.2602352941404642e+24\n",
      "Loss 1.2551990447779654e+24\n",
      "Loss 1.2501832597721733e+24\n",
      "Loss 1.2451876508927118e+24\n",
      "Loss 1.2402119299092048e+24\n",
      "Loss 1.2352560968216523e+24\n",
      "Loss 1.230319863399678e+24\n",
      "Loss 1.2254035178736583e+24\n",
      "Loss 1.220506916128405e+24\n",
      "Loss 1.21562991404873e+24\n",
      "Loss 1.210772079289069e+24\n",
      "Loss 1.2059339162525804e+24\n",
      "Loss 1.2011148484785118e+24\n",
      "Loss 1.1963152362548335e+24\n",
      "Loss 1.1915345751783872e+24\n",
      "Loss 1.1867735137675192e+24\n",
      "Loss 1.1820309711583189e+24\n",
      "Loss 1.1773075958691327e+24\n",
      "Loss 1.1726030276119905e+24\n",
      "Loss 1.1679174825596742e+24\n",
      "Loss 1.1632504563090257e+24\n",
      "Loss 1.158602020917639e+24\n",
      "Loss 1.1539723205007021e+24\n",
      "Loss 1.1493607785974628e+24\n",
      "Loss 1.1447681157838614e+24\n",
      "Loss 1.1401937555991457e+24\n",
      "Loss 1.1356374098129394e+24\n",
      "Loss 1.1310995107708068e+24\n",
      "Loss 1.1265796261271838e+24\n",
      "Loss 1.1220778999972583e+24\n",
      "Loss 1.117593755920278e+24\n",
      "Loss 1.1131283468177476e+24\n",
      "Loss 1.1086799433074102e+24\n",
      "Loss 1.1042496983107703e+24\n",
      "Loss 1.0998370353670757e+24\n",
      "Loss 1.0954420985915144e+24\n",
      "Loss 1.0910648879840864e+24\n",
      "Loss 1.0867046829688514e+24\n",
      "Loss 1.0823622041217497e+24\n",
      "Loss 1.0780370190972171e+24\n",
      "Loss 1.0737295602408178e+24\n",
      "Loss 1.0694385305158592e+24\n",
      "Loss 1.065165226959034e+24\n",
      "Loss 1.0609090731095897e+24\n",
      "Loss 1.0566697807371503e+24\n",
      "Loss 1.0524472057265277e+24\n",
      "Loss 1.04824149219291e+24\n",
      "Loss 1.0440527842514853e+24\n",
      "Loss 1.0398809377870654e+24\n",
      "Loss 1.0357253763388981e+24\n",
      "Loss 1.0315868204829238e+24\n",
      "Loss 1.0274641172976377e+24\n",
      "Loss 1.0233585638197328e+24\n",
      "Loss 1.0192690791852982e+24\n",
      "Loss 1.0151963119126805e+24\n",
      "Loss 1.0111393252531571e+24\n",
      "Loss 1.0070989118402624e+24\n",
      "Loss 1.0030745672708382e+24\n",
      "Loss 9.990663636024784e+23\n",
      "Loss 9.95073940547213e+23\n",
      "Loss 9.910979466233882e+23\n",
      "Loss 9.871372289094995e+23\n",
      "Loss 9.831929403270514e+23\n",
      "Loss 9.792639279545393e+23\n",
      "Loss 9.753507682527156e+23\n",
      "Loss 9.714533171063921e+23\n",
      "Loss 9.675714304003809e+23\n",
      "Loss 9.637048919618997e+23\n",
      "Loss 9.598538459061367e+23\n",
      "Loss 9.560181481179037e+23\n",
      "Loss 9.52197942712389e+23\n",
      "Loss 9.483931576319983e+23\n",
      "Loss 9.446030723007914e+23\n",
      "Loss 9.408286234674906e+23\n",
      "Loss 9.370691626137498e+23\n",
      "Loss 9.333244735667868e+23\n",
      "Loss 9.295949166145717e+23\n",
      "Loss 9.258802035267285e+23\n",
      "Loss 9.22180334303257e+23\n",
      "Loss 9.184951648289693e+23\n",
      "Loss 9.148250553918355e+23\n",
      "Loss 9.111694295311034e+23\n",
      "Loss 9.075284313619609e+23\n",
      "Loss 9.0390191676922e+23\n",
      "Loss 9.002899578104749e+23\n",
      "Loss 8.966922662553492e+23\n",
      "Loss 8.931094185645954e+23\n",
      "Loss 8.895405500470849e+23\n",
      "Loss 8.859856607028177e+23\n",
      "Loss 8.824454711077343e+23\n",
      "Loss 8.789192606858943e+23\n",
      "Loss 8.754070294372976e+23\n",
      "Loss 8.719090655923204e+23\n",
      "Loss 8.684248647478045e+23\n",
      "Loss 8.64954643076532e+23\n",
      "Loss 8.614981844057206e+23\n",
      "Loss 8.580559931385288e+23\n",
      "Loss 8.546269884110459e+23\n",
      "Loss 8.512118187416183e+23\n",
      "Loss 8.4781055618784e+23\n",
      "Loss 8.444226963465528e+23\n",
      "Loss 8.410482392177567e+23\n",
      "Loss 8.376874730318277e+23\n",
      "Loss 8.343401816159838e+23\n",
      "Loss 8.31006148797443e+23\n",
      "Loss 8.27685302518611e+23\n",
      "Loss 8.243780030674582e+23\n",
      "Loss 8.210836739832322e+23\n",
      "Loss 8.178026755539032e+23\n",
      "Loss 8.145347195490951e+23\n",
      "Loss 8.112798059688079e+23\n",
      "Loss 8.080377906978534e+23\n",
      "Loss 8.048088899090139e+23\n",
      "Loss 8.015928874295071e+23\n",
      "Loss 7.983897112017391e+23\n",
      "Loss 7.951993612257098e+23\n",
      "Loss 7.920216933862312e+23\n",
      "Loss 7.888567797408974e+23\n",
      "Loss 7.857045482321142e+23\n",
      "Loss 7.825648547446936e+23\n",
      "Loss 7.794376992786356e+23\n",
      "Loss 7.763230818339402e+23\n",
      "Loss 7.732207141802312e+23\n",
      "Loss 7.70131028663073e+23\n",
      "Loss 7.670535929369011e+23\n",
      "Loss 7.639884070017158e+23\n",
      "Loss 7.609356149727049e+23\n",
      "Loss 7.578948565618984e+23\n",
      "Loss 7.548662038268902e+23\n",
      "Loss 7.518498008828685e+23\n",
      "Loss 7.488453594994571e+23\n",
      "Loss 7.4585295173425e+23\n",
      "Loss 7.428725055296533e+23\n",
      "Loss 7.399040208856668e+23\n",
      "Loss 7.369473536871025e+23\n",
      "Loss 7.340023598187724e+23\n",
      "Loss 7.310693275110526e+23\n",
      "Loss 7.28147968533567e+23\n",
      "Loss 7.252383549439094e+23\n",
      "Loss 7.22340270569298e+23\n",
      "Loss 7.194537874673267e+23\n",
      "Loss 7.165788335804015e+23\n",
      "Loss 7.137154809661163e+23\n",
      "Loss 7.108634413940951e+23\n",
      "Loss 7.08022858979526e+23\n",
      "Loss 7.051935896072208e+23\n",
      "Loss 7.023757053347736e+23\n",
      "Loss 6.995690620469963e+23\n",
      "Loss 6.967735156287008e+23\n",
      "Loss 6.939892101950753e+23\n",
      "Loss 6.912160016309316e+23\n",
      "Loss 6.884538899362697e+23\n",
      "Loss 6.857029471686838e+23\n",
      "Loss 6.829628130402035e+23\n",
      "Loss 6.80233703723611e+23\n",
      "Loss 6.775154751037182e+23\n",
      "Loss 6.748081992381192e+23\n",
      "Loss 6.721116599540319e+23\n",
      "Loss 6.694259293090503e+23\n",
      "Loss 6.667508631879862e+23\n",
      "Loss 6.640865336484338e+23\n",
      "Loss 6.61432868632799e+23\n",
      "Loss 6.587897960834879e+23\n",
      "Loss 6.561573160005002e+23\n",
      "Loss 6.535353563262422e+23\n",
      "Loss 6.509238450031196e+23\n",
      "Loss 6.483226379159444e+23\n",
      "Loss 6.457320232950928e+23\n",
      "Loss 6.431515687950006e+23\n",
      "Loss 6.405816347036379e+23\n",
      "Loss 6.380218607330345e+23\n",
      "Loss 6.354723909983786e+23\n",
      "Loss 6.32933081384482e+23\n",
      "Loss 6.304037157185627e+23\n",
      "Loss 6.278847263461848e+23\n",
      "Loss 6.253756809217841e+23\n",
      "Loss 6.228767235605488e+23\n",
      "Loss 6.203876380896967e+23\n",
      "Loss 6.179085686244158e+23\n",
      "Loss 6.154393710495181e+23\n",
      "Loss 6.129801174225977e+23\n",
      "Loss 6.105306636284664e+23\n",
      "Loss 6.080910817247183e+23\n",
      "Loss 6.056610834809773e+23\n",
      "Loss 6.0324084904122835e+23\n",
      "Loss 6.0083030634787755e+23\n",
      "Loss 5.984293473145338e+23\n",
      "Loss 5.960379719411971e+23\n",
      "Loss 5.9365625228546144e+23\n",
      "Loss 5.9128390011695074e+23\n",
      "Loss 5.889212036660411e+23\n",
      "Loss 5.865678386735594e+23\n",
      "Loss 5.842239132258967e+23\n",
      "Loss 5.8188931923666185e+23\n",
      "Loss 5.7956409273465195e+23\n",
      "Loss 5.7724816166227295e+23\n",
      "Loss 5.749414899907278e+23\n",
      "Loss 5.726440056624225e+23\n",
      "Loss 5.7035570867735706e+23\n",
      "Loss 5.680766710931255e+23\n",
      "Loss 5.658066046793516e+23\n",
      "Loss 5.635456895800206e+23\n",
      "Loss 5.6129378167994426e+23\n",
      "Loss 5.5905080892152866e+23\n",
      "Loss 5.5681673527597674e+23\n",
      "Loss 5.5459184897366466e+23\n",
      "Loss 5.5237564561143415e+23\n",
      "Loss 5.5016837739086434e+23\n",
      "Loss 5.479697921103761e+23\n",
      "Loss 5.457801780003456e+23\n",
      "Loss 5.4359928285919366e+23\n",
      "Loss 5.4142717874451436e+23\n",
      "Loss 5.392635053683375e+23\n",
      "Loss 5.371086950762273e+23\n",
      "Loss 5.349624596378076e+23\n",
      "Loss 5.328246909666874e+23\n",
      "Loss 5.306955692068517e+23\n",
      "Loss 5.285747700991274e+23\n",
      "Loss 5.264626179026877e+23\n",
      "Loss 5.2435904055993845e+23\n",
      "Loss 5.222636057253155e+23\n",
      "Loss 5.20176601629195e+23\n",
      "Loss 5.1809810032917096e+23\n",
      "Loss 5.160277415372732e+23\n",
      "Loss 5.139657414262839e+23\n",
      "Loss 5.119118838234208e+23\n",
      "Loss 5.098663128438721e+23\n",
      "Loss 5.078290284876378e+23\n",
      "Loss 5.057996704667476e+23\n",
      "Loss 5.037785270115778e+23\n",
      "Loss 5.017653098917521e+23\n",
      "Loss 4.997603433664438e+23\n",
      "Loss 4.977632311188856e+23\n",
      "Loss 4.957741532930626e+23\n",
      "Loss 4.937930738601779e+23\n",
      "Loss 4.918198847338403e+23\n",
      "Loss 4.898544778276588e+23\n",
      "Loss 4.878971053432125e+23\n",
      "Loss 4.859475150789223e+23\n",
      "Loss 4.840056349771942e+23\n",
      "Loss 4.820716091532162e+23\n",
      "Loss 4.801451493766122e+23\n",
      "Loss 4.782266159353524e+23\n",
      "Loss 4.763156485414665e+23\n",
      "Loss 4.7441221116615764e+23\n",
      "Loss 4.725164839534108e+23\n",
      "Loss 4.7062828675924095e+23\n",
      "Loss 4.6874761958364805e+23\n",
      "Loss 4.668744824266321e+23\n",
      "Loss 4.650089473457872e+23\n",
      "Loss 4.631507261107371e+23\n",
      "Loss 4.6129999886546695e+23\n",
      "Loss 4.5945654943719464e+23\n",
      "Loss 4.576206300274993e+23\n",
      "Loss 4.557919884348018e+23\n",
      "Loss 4.539706606878991e+23\n",
      "Loss 4.5215653870040024e+23\n",
      "Loss 4.503498386450873e+23\n",
      "Loss 4.48550128176396e+23\n",
      "Loss 4.467576955247025e+23\n",
      "Loss 4.449723965748188e+23\n",
      "Loss 4.431943033843389e+23\n",
      "Loss 4.414234159532628e+23\n",
      "Loss 4.396594460512143e+23\n",
      "Loss 4.379026098509756e+23\n",
      "Loss 4.3615276323735856e+23\n",
      "Loss 4.344098701815662e+23\n",
      "Loss 4.326740027411925e+23\n",
      "Loss 4.309450168010494e+23\n",
      "Loss 4.29222912361137e+23\n",
      "Loss 4.2750765339265815e+23\n",
      "Loss 4.2579931195320696e+23\n",
      "Loss 4.240978520139864e+23\n",
      "Loss 4.2240309343101134e+23\n",
      "Loss 4.207152163482669e+23\n",
      "Loss 4.190341847369561e+23\n",
      "Loss 4.173596383091087e+23\n",
      "Loss 4.1569190132389786e+23\n",
      "Loss 4.140307936373385e+23\n",
      "Loss 4.123763512782277e+23\n",
      "Loss 4.107285382177684e+23\n",
      "Loss 4.090871382831784e+23\n",
      "Loss 4.07452511762428e+23\n",
      "Loss 4.05824298367547e+23\n",
      "Loss 4.0420271427131746e+23\n",
      "Loss 4.025874712433633e+23\n",
      "Loss 4.009787494276695e+23\n",
      "Loss 3.993763686802511e+23\n"
     ]
    }
   ],
   "source": [
    "# Task 6\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "def save_model(best_model, best_hyperparameters, best_metrics):\n",
    "    '''\n",
    "        Creates a models folder, then within the models' folder creates a regression folder and finally creates a last neural networks folder where it stores the model, a dictionary of its hyperparameters and a dictionary of its metrics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_name: str\n",
    "            A string used to name the folder to be created\n",
    "        \n",
    "        best_model: pytorch model\n",
    "            A model from pythorch\n",
    "        \n",
    "        best_hyperparameters: dict\n",
    "            A dictionary containing the optimal hyperparameters configuration\n",
    "        \n",
    "        best_metrics: dict \n",
    "            A dictionary containing the test metrics obtained using the best model   \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None       \n",
    "    '''\n",
    "\n",
    "    # Create Models folder\n",
    "    models_dir = 'airbnb-property-listings/models'\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    models_path = os.path.join(current_dir, models_dir)\n",
    "    if os.path.exists(models_path) == False:\n",
    "        os.mkdir(models_path)\n",
    "\n",
    "    # Create regression folder\n",
    "    regression_dir = 'airbnb-property-listings/models/regression'\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    regression_path = os.path.join(current_dir, regression_dir)\n",
    "    if os.path.exists(regression_path) == False:\n",
    "        os.mkdir(regression_path)\n",
    "\n",
    "    # Create neural_networks folder\n",
    "    nn_name_dir = os.path.join(regression_path,'neural_networks') # Create the neural network folder\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    nn_name_path = os.path.join(current_dir, nn_name_dir)\n",
    "    if os.path.exists(nn_name_path) == False:\n",
    "        os.mkdir(nn_name_path)\n",
    "\n",
    "    # Create a Timestamp folder\n",
    "    timestamp_dir = os.path.join(nn_name_dir,time.strftime(\"%Y-%m-%d_%H:%M:%S\")) # Create the timestamp folder\n",
    "    current_dir = os.path.dirname(os.getcwd())\n",
    "    timestamp_path = os.path.join(current_dir, timestamp_dir)\n",
    "    if os.path.exists(timestamp_path) == False:\n",
    "        os.mkdir(timestamp_path)\n",
    "\n",
    "    # Save the model in a file called model.pt\n",
    "    torch.save(best_model, os.path.join(timestamp_path, 'model.pt')) \n",
    "\n",
    "    # Save the hyperparameters in a file called hyperparameters.json\n",
    "    with open(os.path.join(timestamp_path, 'hyperparameters.json'), 'w') as fp: \n",
    "            json.dump(best_hyperparameters, fp)\n",
    "\n",
    "    # Save the metrics in a file called metrics.json\n",
    "    with open(os.path.join(timestamp_path, 'metrics.json'), 'w') as fp:\n",
    "            json.dump(best_metrics, fp)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Define the model\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "best_model = train(model,train_loader,nn_config)\n",
    "\n",
    "# Define the hyperparemeters\n",
    "\n",
    "best_hyperparameters = get_nn_config()\n",
    "\n",
    "# Calculate the metrics\n",
    "\n",
    "''' \n",
    "The RMSE loss of your model under a key called RMSE_loss for training, validation, and test sets\n",
    "The R^2 score of your model under a key called R_squared for training, validation, and test sets\n",
    "The time taken to train the model under a key called training_duration\n",
    "The average time taken to make a prediction under a key called inference_latency\n",
    "\n",
    "'''\n",
    "best_metrics = {\n",
    "\n",
    "    'RMSE_loss' : 0, #[training, validation, test], # Need to Calculate the metrics\n",
    "    'R^2' : 0, #[training, validation, test],\n",
    "    'training_duration' : 0,\n",
    "    'inference_latency' : 0,\n",
    "}\n",
    "\n",
    "save_model(best_model, best_hyperparameters, best_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7\n",
    "\n",
    "import itertools\n",
    "# Define a fucntion wich creates many config dictionaries for your network \n",
    "\n",
    "def generate_nn_configs():\n",
    "\n",
    "    # Parameters to change are: Optimiser, lr, hidden_layer_width and depth\n",
    "    combinations_dict = {\n",
    "        'Optimisers':['SGD', 'Adam', 'Adagrad'],\n",
    "        'lr':[0.001, 0.0001],\n",
    "        'hidden_layer_width':[32, 64, 128, 256],\n",
    "        'depth':[3,5,10]\n",
    "    }\n",
    "\n",
    "    config_dict_list = []\n",
    "    # For every possible combination of the combinations_dict create a custom dictionary that is later stored in config_dict_list\n",
    "    for iteration in itertools.product(*combinations_dict.values()):\n",
    "        config_dict = {\n",
    "            'optimiser': iteration[0],\n",
    "            'lr': iteration[1],\n",
    "            'hidden_layer_width': iteration[2],\n",
    "            'depth': iteration[3]\n",
    "        }\n",
    "        config_dict_list.append(config_dict)\n",
    "\n",
    "    return config_dict_list\n",
    "\n",
    "def find_best_nn():\n",
    "    # Call the previous function to get the list of config dictionaries\n",
    "    config_dict_list = generate_nn_configs()\n",
    "\n",
    "    # For each configuration, redefine the nn_model and the training function\n",
    "    for nn_config in config_dict_list():\n",
    "\n",
    "        # Redefine NeuralNetwork to include the custom numbers of hidden layers (depth) and hidden layers width\n",
    "        class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                # Initialise the Parameters\n",
    "                #self.linear_layer = torch.nn.Linear(11,1) # 11 features, 1 label\n",
    "\n",
    "                self.layers = torch.nn.Sequential()\n",
    "                self.layers.add_module(\"Input Layer\", torch.nn.Linear(11, nn_config['hidden_layer_width'])) # Input layer\n",
    "                self.layers.add_module(\"ReLU\", torch.nn.ReLU())\n",
    "                for i in range(nn_config['depth'] - 2): #  The input and the first linear layer are already taken into account\n",
    "                    self.layers.add_module(\"Hidden Layer\", torch.nn.Linear(nn_config['hidden_layer_width'], nn_config['hidden_layer_width'])) # Hidden Layer\n",
    "                    self.layers.add_module(\"Hidden ReLU\", torch.nn.ReLU())\n",
    "                self.layers.add_module(\"Output Layer\", torch.nn.Linear(nn_config['hidden_layer_width'], 1))# output layer\n",
    "            \n",
    "            def forward(self, features):\n",
    "                # Use the layers to process the features\n",
    "                return self.layers(features)\n",
    "\n",
    "        model = NeuralNetwork()\n",
    "\n",
    "        # Train function with config \n",
    "        def train(model, dataloader, nn_config, epochs=12):\n",
    "\n",
    "            # Set optimiser with lr from nn_config\n",
    "            if nn_config['optimiser'] == \"SGD\":\n",
    "                optimiser = torch.optim.SGD(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "            elif nn_config['optimiser'] == \"Adam\":\n",
    "                optimiser = torch.optim.Adam(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "            elif nn_config['optimiser'] == \"Adagrad\":\n",
    "                optimiser = torch.optim.Adagrad(model.parameters(), lr=nn_config['lr'])\n",
    "\n",
    "            writer = SummaryWriter()\n",
    "\n",
    "            batch_idx = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                batch_idx = 0\n",
    "                current_loss = 0.0\n",
    "                for batch in dataloader:\n",
    "                    features, labels = batch\n",
    "                    features = features.to(torch.float32) # Convert torch into the right format\n",
    "                    labels = labels.to(torch.float32) # Convert torch into the right format\n",
    "                    prediction = model(features)\n",
    "                    loss = loss_fn(prediction,labels)\n",
    "                    loss.backward() \n",
    "                    optimiser.step() # Optimiser step\n",
    "                    optimiser.zero_grad()\n",
    "                    ls = loss.item()\n",
    "                    print(\"Loss\", ls)\n",
    "                    batch_idx += 1\n",
    "                    current_loss = current_loss + ls\n",
    "                \n",
    "                # print (f\"currentnt loss {current_loss} and batch index {batch_idx}\")\n",
    "                # print(f'Loss after mini-batch  ({epoch + 1} : {current_loss // batch_idx}')\n",
    "                    writer.add_scalar('Loss - Task 7',current_loss / batch_idx , epoch)\n",
    "\n",
    "        train(model,train_loader,nn_config)\n",
    "\n",
    "        # Calculate metrics\n",
    "\n",
    "        # ''' \n",
    "        # The RMSE loss of your model under a key called RMSE_loss for training, validation, and test sets\n",
    "        # The R^2 score of your model under a key called R_squared for training, validation, and test sets\n",
    "        # The time taken to train the model under a key called training_duration\n",
    "        # The average time taken to make a prediction under a key called inference_latency\n",
    "\n",
    "        # '''\n",
    "        # best_metrics = {\n",
    "\n",
    "        #     'RMSE_loss' : 0, #[training, validation, test], # Need to Calculate the metrics\n",
    "        #     'R^2' : 0, #[training, validation, test],\n",
    "        #     'training_duration' : 0,\n",
    "        #     'inference_latency' : 0,\n",
    "        # }\n",
    "\n",
    "        # STore the metrics, config, and model:\n",
    "\n",
    "        # if metrics are best than they were:  WHAT SHOULD I LOOK AT (RMSE)\n",
    "        #     best_model = best_model\n",
    "        #     best_hyperparameters = best_hyperparameters\n",
    "        #     best_metrics = best_metrics\n",
    "\n",
    "    # save_model(best_model, best_hyperparameters, best_metrics)=\n",
    "        \n",
    "    return \n",
    "\n",
    "# find_best_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = generate_nn_configs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.NeuralNetwork"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(NeuralNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yesss\n"
     ]
    }
   ],
   "source": [
    "if type(model) == type(NeuralNetwork()):\n",
    "    print('yesss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b64a52f329bd7ac83fd7355bf526de5da9b8503da8fcdfb6f417a855f85dd2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
